{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h9464I-uxLiw"
   },
   "source": [
    "# Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IvLvmewlxLix"
   },
   "source": [
    "<font face='georgia'>\n",
    "    \n",
    "   <h4><strong>What does tf-idf mean?</strong></h4>\n",
    "\n",
    "   <p>    \n",
    "Tf-idf stands for <em>term frequency-inverse document frequency</em>, and the tf-idf weight is a weight often used in information retrieval and text mining. This weight is a statistical measure used to evaluate how important a word is to a document in a collection or corpus. The importance increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus. Variations of the tf-idf weighting scheme are often used by search engines as a central tool in scoring and ranking a document's relevance given a user query.\n",
    "</p>\n",
    "    \n",
    "   <p>\n",
    "One of the simplest ranking functions is computed by summing the tf-idf for each query term; many more sophisticated ranking functions are variants of this simple model.\n",
    "</p>\n",
    "    \n",
    "   <p>\n",
    "Tf-idf can be successfully used for stop-words filtering in various subject fields including text summarization and classification.\n",
    "</p>\n",
    "    \n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4XLaGFUMxLiy"
   },
   "source": [
    "<font face='georgia'>\n",
    "    <h4><strong>How to Compute:</strong></h4>\n",
    "\n",
    "Typically, the tf-idf weight is composed by two terms: the first computes the normalized Term Frequency (TF), aka. the number of times a word appears in a document, divided by the total number of words in that document; the second term is the Inverse Document Frequency (IDF), computed as the logarithm of the number of the documents in the corpus divided by the number of documents where the specific term appears.\n",
    "\n",
    " <ul>\n",
    "    <li>\n",
    "<strong>TF:</strong> Term Frequency, which measures how frequently a term occurs in a document. Since every document is different in length, it is possible that a term would appear much more times in long documents than shorter ones. Thus, the term frequency is often divided by the document length (aka. the total number of terms in the document) as a way of normalization: <br>\n",
    "\n",
    "$TF(t) = \\frac{\\text{Number of times term t appears in a document}}{\\text{Total number of terms in the document}}.$\n",
    "</li>\n",
    "<li>\n",
    "<strong>IDF:</strong> Inverse Document Frequency, which measures how important a term is. While computing TF, all terms are considered equally important. However it is known that certain terms, such as \"is\", \"of\", and \"that\", may appear a lot of times but have little importance. Thus we need to weigh down the frequent terms while scale up the rare ones, by computing the following: <br>\n",
    "\n",
    "$IDF(t) = \\log_{e}\\frac{\\text{Total  number of documents}} {\\text{Number of documents with term t in it}}.$\n",
    "for numerical stabiltiy we will be changing this formula little bit\n",
    "$IDF(t) = \\log_{e}\\frac{\\text{Total  number of documents}} {\\text{Number of documents with term t in it}+1}.$\n",
    "</li>\n",
    "</ul>\n",
    "\n",
    "<br>\n",
    "<h4><strong>Example</strong></h4>\n",
    "<p>\n",
    "\n",
    "Consider a document containing 100 words wherein the word cat appears 3 times. The term frequency (i.e., tf) for cat is then (3 / 100) = 0.03. Now, assume we have 10 million documents and the word cat appears in one thousand of these. Then, the inverse document frequency (i.e., idf) is calculated as log(10,000,000 / 1,000) = 4. Thus, the Tf-idf weight is the product of these quantities: 0.03 * 4 = 0.12.\n",
    "</p>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dg2ooa4DxLiz"
   },
   "source": [
    "## Task-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TAcTjhO8xLiz"
   },
   "source": [
    "<font face='georgia'>\n",
    "    <h4><strong>1. Build a TFIDF Vectorizer & compare its results with Sklearn:</strong></h4>\n",
    "\n",
    "<ul>\n",
    "    <li> As a part of this task you will be implementing TFIDF vectorizer on a collection of text documents.</li>\n",
    "    <br>\n",
    "    <li> You should compare the results of your own implementation of TFIDF vectorizer with that of sklearns implemenation TFIDF vectorizer.</li>\n",
    "    <br>\n",
    "    <li> Sklearn does few more tweaks in the implementation of its version of TFIDF vectorizer, so to replicate the exact results you would need to add following things to your custom implementation of tfidf vectorizer:\n",
    "       <ol>\n",
    "        <li> Sklearn has its vocabulary generated from idf sroted in alphabetical order</li>\n",
    "        <li> Sklearn formula of idf is different from the standard textbook formula. Here the constant <strong>\"1\"</strong> is added to the numerator and denominator of the idf as if an extra document was seen containing every term in the collection exactly once, which prevents zero divisions.\n",
    "            \n",
    " $IDF(t) = 1+\\log_{e}\\frac{1\\text{ }+\\text{ Total  number of documents in collection}} {1+\\text{Number of documents with term t in it}}.$\n",
    "        </li>\n",
    "        <li> Sklearn applies L2-normalization on its output matrix.</li>\n",
    "        <li> The final output of sklearn tfidf vectorizer is a sparse matrix.</li>\n",
    "    </ol>\n",
    "    <br>\n",
    "    <li>Steps to approach this task:\n",
    "    <ol>\n",
    "        <li> You would have to write both fit and transform methods for your custom implementation of tfidf vectorizer.</li>\n",
    "        <li> Print out the alphabetically sorted voacb after you fit your data and check if its the same as that of the feature names from sklearn tfidf vectorizer. </li>\n",
    "        <li> Print out the idf values from your implementation and check if its the same as that of sklearns tfidf vectorizer idf values. </li>\n",
    "        <li> Once you get your voacb and idf values to be same as that of sklearns implementation of tfidf vectorizer, proceed to the below steps. </li>\n",
    "        <li> Make sure the output of your implementation is a sparse matrix. Before generating the final output, you need to normalize your sparse matrix using L2 normalization. You can refer to this link https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.normalize.html </li>\n",
    "        <li> After completing the above steps, print the output of your custom implementation and compare it with sklearns implementation of tfidf vectorizer.</li>\n",
    "        <li> To check the output of a single document in your collection of documents,  you can convert the sparse matrix related only to that document into dense matrix and print it.</li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <br>\n",
    "   </ul>\n",
    "\n",
    "  <p> <font color=\"#e60000\"><strong>Note-1: </strong></font> All the necessary outputs of sklearns tfidf vectorizer have been provided as reference in this notebook, you can compare your outputs as mentioned in the above steps, with these outputs.<br>\n",
    "   <font color=\"#e60000\"><strong>Note-2: </strong></font> The output of your custom implementation and that of sklearns implementation would match only with the collection of document strings provided to you as reference in this notebook. It would not match for strings that contain capital letters or punctuations, etc, because sklearn version of tfidf vectorizer deals with such strings in a different way. To know further details about how sklearn tfidf vectorizer works with such string, you can always refer to its official documentation.<br>\n",
    "   <font color=\"#e60000\"><strong>Note-3: </strong></font> During this task, it would be helpful for you to debug the code you write with print statements wherever necessary. But when you are finally submitting the assignment, make sure your code is readable and try not to print things which are not part of this task.\n",
    "    </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OnV82tg1xLi0"
   },
   "source": [
    "### Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bUsYm9wjxLi1"
   },
   "outputs": [],
   "source": [
    "## SkLearn# Collection of string documents\n",
    "\n",
    "corpus = [\n",
    "     'this is the first document',\n",
    "     'this document is the second document',\n",
    "     'and this is the third one',\n",
    "     'is this the first document',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eLwmFZfKxLi4"
   },
   "source": [
    "### SkLearn Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Np4dfQOkxLi4"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(corpus)\n",
    "skl_output = vectorizer.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-7Om8YpYxLi6",
    "outputId": "0a3bd0f5-4424-4400-944f-4482a80bd799"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"
     ]
    }
   ],
   "source": [
    "# sklearn feature names, they are sorted in alphabetic order by default.\n",
    "\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dTKplK96xLi-",
    "outputId": "53722fa2-6756-4aa0-f179-37b578bb6890"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.91629073 1.22314355 1.51082562 1.         1.91629073 1.91629073\n",
      " 1.         1.91629073 1.        ]\n"
     ]
    }
   ],
   "source": [
    "# Here we will print the sklearn tfidf vectorizer idf values after applying the fit method\n",
    "# After using the fit function on the corpus the vocab has 9 words in it, and each has its idf value.\n",
    "\n",
    "print(vectorizer.idf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-CTiWHygxLjA",
    "outputId": "8d5a9cde-2c29-4afe-f7b4-1547e88dba4f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 9)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of sklearn tfidf vectorizer output after applying transform method.\n",
    "\n",
    "skl_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bDKEpbA-xLjD",
    "outputId": "87dafd65-5313-443f-8c6e-1b05cc8c2543"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 8)\t0.38408524091481483\n",
      "  (0, 6)\t0.38408524091481483\n",
      "  (0, 3)\t0.38408524091481483\n",
      "  (0, 2)\t0.5802858236844359\n",
      "  (0, 1)\t0.46979138557992045\n"
     ]
    }
   ],
   "source": [
    "# sklearn tfidf values for first line of the above corpus.\n",
    "# Here the output is a sparse matrix\n",
    "# [array([[0.38408524, 0.38408524, 0.38408524, 0.58028582, 0.46979139]]), array([[0.32167263, 0.39345181, 0.32167263, 0.32167263, 0.61641829,\n",
    "#         0.39345181]]), array([[0.51184851, 0.26710379, 0.26710379, 0.26710379, 0.51184851,\n",
    "#         0.51184851]]), array([[0.38408524, 0.38408524, 0.38408524, 0.58028582, 0.46979139]])]\n",
    "\n",
    "print(skl_output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3QWo34hexLjF",
    "outputId": "cdc04e08-989f-4bdc-dd7f-f1c82a9f90be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.6876236  0.         0.28108867 0.         0.53864762\n",
      "  0.28108867 0.         0.28108867]]\n"
     ]
    }
   ],
   "source": [
    "# sklearn tfidf values for first line of the above corpus.\n",
    "# To understand the output better, here we are converting the sparse output matrix to dense matrix and printing it.\n",
    "# Notice that this output is normalized using L2 normalization. sklearn does this by default.\n",
    "\n",
    "print(skl_output[1].toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qfIwx5LzxLjI"
   },
   "source": [
    "### Your custom implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HjuCcJwXxLjJ"
   },
   "outputs": [],
   "source": [
    "# Write your code here.\n",
    "# Make sure its well documented and readble with appropriate comments.\n",
    "# Compare your results with the above sklearn tfidf vectorizer\n",
    "# You are not supposed to use any other library apart from the ones given below\n",
    "\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix\n",
    "import math\n",
    "import operator\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_word=[]\n",
    "final_tfidf=[]\n",
    "def fit_tfidf(data):\n",
    "    \n",
    "    row_count=0\n",
    "    unique_words=set()\n",
    "    #check if the input data is of type ist or not \n",
    "    if isinstance(data,(list,)):\n",
    "        for row in data:\n",
    "            for word in row.split(' '):\n",
    "                if len(word)<2:\n",
    "                    continue\n",
    "                unique_words.add(word)\n",
    "            row_count+=1\n",
    "    else:\n",
    "        print('Input Data is Not of type list ')\n",
    "#     print(row_count)\n",
    "#     print(unique_words)\n",
    "    unique_words=sorted(list(unique_words))\n",
    "#     print(unique_words)\n",
    "    if isinstance(data,(list,)):\n",
    "        for word in unique_words:\n",
    "            tfidf=0\n",
    "            freq_word_row=0\n",
    "            for row in data:\n",
    "                if word in row:\n",
    "                    \n",
    "                    freq_word_row+=1\n",
    "#                     print(word)\n",
    "#                     print(row)\n",
    "#                     print(freq_word_row)\n",
    "#             print(1+row_count)\n",
    "#             print(1+freq_word_row)\n",
    "            tfidf=1+(np.log((1+row_count)/(1+freq_word_row)))\n",
    "            final_word.append(word)\n",
    "            print(tfidf)\n",
    "            final_tfidf.append(tfidf)\n",
    "           \n",
    "    print(unique_words)\n",
    "#     print(*final_tfidf)\n",
    "\n",
    "                \n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.916290731874155\n",
      "1.2231435513142097\n",
      "1.5108256237659907\n",
      "1.0\n",
      "1.916290731874155\n",
      "1.916290731874155\n",
      "1.0\n",
      "1.916290731874155\n",
      "1.0\n",
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "     'this is the first document',\n",
    "     'this document is the second document',\n",
    "     'and this is the third one',\n",
    "     'is this the first document',\n",
    "]\n",
    "fit_tfidf(corpus)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and document first is one second the third this\n",
      "1.916290731874155 1.2231435513142097 1.5108256237659907 1.0 1.916290731874155 1.916290731874155 1.0 1.916290731874155 1.0\n"
     ]
    }
   ],
   "source": [
    "print(*final_word)\n",
    "print(*final_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_val=dict(zip(final_word,final_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3, 4]]\n"
     ]
    }
   ],
   "source": [
    "normalized_value=[]\n",
    "l1=[1,2,3,4]\n",
    "normalized_value.append(l1)\n",
    "print(normalized_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this\n",
      "is\n",
      "the\n",
      "first\n",
      "document\n",
      "this\n",
      "document\n",
      "is\n",
      "the\n",
      "second\n",
      "document\n",
      "and\n",
      "this\n",
      "is\n",
      "the\n",
      "third\n",
      "one\n",
      "is\n",
      "this\n",
      "the\n",
      "first\n",
      "document\n"
     ]
    }
   ],
   "source": [
    "for row in corpus:\n",
    "    for word in row.split(' '):\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "global normalized_value\n",
    "normalized_value=[]\n",
    "global final_normalized_input\n",
    "final_normalized_input=[]\n",
    "rows = []\n",
    "columns = []\n",
    "values = []\n",
    "final_output=[]\n",
    "output=[]\n",
    "\n",
    "def tfidf_transform(data,vocab):\n",
    "    prev_idx=0\n",
    "    if isinstance(data,(list,)):\n",
    "        for idx,row in enumerate(tqdm(data)):\n",
    "            word_column=0\n",
    "            for word in row.split(' '):\n",
    "                word_column+=1\n",
    "                key_value=word_val.get(word,0)\n",
    "                \n",
    "#                 print(normalized_value)\n",
    "#                 print(idx)\n",
    "#                 print(prev_idx)\n",
    "                if(idx!=prev_idx):\n",
    "#                     print(\"normal\",normalized_value)\n",
    "                    final_normalized_input.append(normalized_value[:])\n",
    "#                     print(final_normalized_input)\n",
    "#                     print(\"prev\",prev_idx)\n",
    "#                     print(\"current\",idx)\n",
    "            \n",
    "                    prev_idx=idx\n",
    "                    normalized_value.clear()\n",
    "                normalized_value.append(key_value)\n",
    "#                 print(\"outside_normal\",normalized_value)\n",
    "        final_normalized_input.append(normalized_value[:])\n",
    "                    \n",
    "                    \n",
    "# #                 rows.append(idx)\n",
    "# #                 columns.append(word_column)\n",
    "#                 normalized_value.append(key_value)\n",
    "# #                 print(word)\n",
    "#                 if(row!=(prev_row)):\n",
    "        \n",
    "        for i in range(len(final_normalized_input)):\n",
    "            output=normalize([final_normalized_input[i]])\n",
    "            final_output.append(output[:])\n",
    "            \n",
    "        j=0\n",
    "        for i in range(len(final_output)):\n",
    "            for k in range(len(final_output)):\n",
    "                print(i,j,k,final_output[i][j][k])\n",
    "            \n",
    "\n",
    "        \n",
    "                       \n",
    "        \n",
    "        \n",
    "#                 prev_row=row\n",
    "                \n",
    "# #                 print(idx,word_column,normalized_value)\n",
    "#                 print(idx)   \n",
    "# #             final_normalized_input.append(normalized_value)\n",
    "# #             print(normalized_value)\n",
    "# # #             output=normalize(final_normalized_input)\n",
    "# #             final_output.append(output)      \n",
    "        \n",
    "        \n",
    "    else:\n",
    "        print(\"input a list\")\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 4004.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0 0.38408524091481483\n",
      "0 0 1 0.38408524091481483\n",
      "0 0 2 0.38408524091481483\n",
      "0 0 3 0.5802858236844359\n",
      "1 0 0 0.3216726331114366\n",
      "1 0 1 0.3934518068245154\n",
      "1 0 2 0.3216726331114366\n",
      "1 0 3 0.3216726331114366\n",
      "2 0 0 0.511848512707169\n",
      "2 0 1 0.267103787642168\n",
      "2 0 2 0.267103787642168\n",
      "2 0 3 0.267103787642168\n",
      "3 0 0 0.38408524091481483\n",
      "3 0 1 0.38408524091481483\n",
      "3 0 2 0.38408524091481483\n",
      "3 0 3 0.5802858236844359\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "     'this is the first document',\n",
    "     'this document is the second document',\n",
    "     'and this is the third one',\n",
    "     'is this the first document',\n",
    "]\n",
    "tfidf_transform(corpus,word_val)\n",
    "# print(*final_normalized_input)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MMxBmVZExLjK"
   },
   "source": [
    "## Task-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "51j_OtqAxLjL"
   },
   "source": [
    "<font face='georgia'>\n",
    "    <h4><strong>2. Implement max features functionality:</strong></h4>\n",
    "\n",
    "<ul>\n",
    "    <li> As a part of this task you have to modify your fit and transform functions so that your vocab will contain only 50 terms with top idf scores.</li>\n",
    "    <br>\n",
    "    <li>This task is similar to your previous task, just that here your vocabulary is limited to only top 50 features names based on their idf values. Basically your output will have exactly 50 columns and the number of rows will depend on the number of documents you have in your corpus.</li>\n",
    "    <br>\n",
    "    <li>Here you will be give a pickle file, with file name <strong>cleaned_strings</strong>. You would have to load the corpus from this file and use it as input to your tfidf vectorizer.</li>\n",
    "    <br>\n",
    "    <li>Steps to approach this task:\n",
    "    <ol>\n",
    "        <li> You would have to write both fit and transform methods for your custom implementation of tfidf vectorizer, just like in the previous task. Additionally, here you have to limit the number of features generated to 50 as described above.</li>\n",
    "        <li> Now sort your vocab based in descending order of idf values and print out the words in the sorted voacb after you fit your data. Here you should be getting only 50 terms in your vocab. And make sure to print idf values for each term in your vocab. </li>\n",
    "        <li> Make sure the output of your implementation is a sparse matrix. Before generating the final output, you need to normalize your sparse matrix using L2 normalization. You can refer to this link https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.normalize.html </li>\n",
    "        <li> Now check the output of a single document in your collection of documents,  you can convert the sparse matrix related only to that document into dense matrix and print it. And this dense matrix should contain 1 row and 50 columns. </li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <br>\n",
    "   </ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NHxPLlwNxLjL",
    "outputId": "9abd8e08-0e24-4975-9a13-4d3636d60323"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in corpus =  746\n"
     ]
    }
   ],
   "source": [
    "# Below is the code to load the cleaned_strings pickle file provided\n",
    "# Here corpus is of list type\n",
    "\n",
    "import pickle\n",
    "with open('cleaned_strings', 'rb') as f:\n",
    "    corpus = pickle.load(f)\n",
    "    \n",
    "# printing the length of the corpus loaded\n",
    "print(\"Number of documents in corpus = \",len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'slow moving aimless movie distressed drifting young man'"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we will be using this document as our test document .\n",
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZULfoOIdxLjQ"
   },
   "outputs": [],
   "source": [
    "# Write your code here.\n",
    "# Try not to hardcode any values.\n",
    "# Make sure its well documented and readble with appropriate comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1_DJnnR3xLjR"
   },
   "outputs": [],
   "source": [
    "#Fit function for or tfidf unique words and their tfidf values\n",
    "final_word=[]\n",
    "final_tfidf=[]\n",
    "def fit_tfidf(data):\n",
    "    \n",
    "    row_count=0\n",
    "    unique_words=set()\n",
    "    #check if the input data is of type ist or not \n",
    "    if isinstance(data,(list,)):\n",
    "        for row in data:\n",
    "            for word in row.split(' '):\n",
    "                if len(word)<2:\n",
    "                    continue\n",
    "                unique_words.add(word)\n",
    "            row_count+=1\n",
    "    else:\n",
    "        print('Input Data is Not of type list ')\n",
    "#     print(row_count)\n",
    "#     print(unique_words)\n",
    "    unique_words=sorted(list(unique_words))\n",
    "#     print(unique_words)\n",
    "    if isinstance(data,(list,)):\n",
    "        for word in unique_words:\n",
    "            tfidf=0\n",
    "            freq_word_row=0\n",
    "            for row in data:\n",
    "                if word in row:\n",
    "                    \n",
    "                    freq_word_row+=1\n",
    "#                     print(word)\n",
    "#                     print(row)\n",
    "#                     print(freq_word_row)\n",
    "#             print(1+row_count)\n",
    "#             print(1+freq_word_row)\n",
    "            tfidf=1+(np.log((1+row_count)/(1+freq_word_row)))\n",
    "            final_word.append(word)\n",
    "            print(word,tfidf)\n",
    "            final_tfidf.append(tfidf)\n",
    "           \n",
    "    print(unique_words)\n",
    "#     print(*final_tfidf)\n",
    "\n",
    "                \n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aailiyah 6.922918004572872\n",
      "abandoned 6.922918004572872\n",
      "ability 6.229770824012927\n",
      "abroad 6.922918004572872\n",
      "absolutely 5.3134800921387715\n",
      "abstruse 6.922918004572872\n",
      "abysmal 6.517452896464707\n",
      "academy 6.922918004572872\n",
      "accents 6.922918004572872\n",
      "accessible 6.922918004572872\n",
      "acclaimed 6.922918004572872\n",
      "accolades 6.922918004572872\n",
      "accurate 6.517452896464707\n",
      "accurately 6.922918004572872\n",
      "accused 6.517452896464707\n",
      "achievement 6.517452896464707\n",
      "achille 6.922918004572872\n",
      "ackerman 6.922918004572872\n",
      "act 2.6960842593046923\n",
      "acted 6.229770824012927\n",
      "acting 3.927185731018881\n",
      "action 5.218169912334447\n",
      "actions 6.229770824012927\n",
      "actor 4.283860674957613\n",
      "actors 4.671626205966376\n",
      "actress 5.670155036077504\n",
      "actresses 6.229770824012927\n",
      "actually 5.218169912334447\n",
      "adams 6.922918004572872\n",
      "adaptation 6.517452896464707\n",
      "add 5.824305715904762\n",
      "added 6.922918004572872\n",
      "addition 6.229770824012927\n",
      "admins 6.922918004572872\n",
      "admiration 6.922918004572872\n",
      "admitted 6.922918004572872\n",
      "adorable 6.006627272698717\n",
      "adrift 6.922918004572872\n",
      "adventure 6.922918004572872\n",
      "advise 6.517452896464707\n",
      "aerial 6.229770824012927\n",
      "aesthetically 6.922918004572872\n",
      "affected 6.922918004572872\n",
      "affleck 6.922918004572872\n",
      "afraid 6.517452896464707\n",
      "africa 6.517452896464707\n",
      "afternoon 6.922918004572872\n",
      "age 4.214867803470662\n",
      "aged 6.229770824012927\n",
      "ages 6.229770824012927\n",
      "ago 5.824305715904762\n",
      "agree 6.517452896464707\n",
      "agreed 6.922918004572872\n",
      "aimless 6.922918004572872\n",
      "air 5.3134800921387715\n",
      "aired 6.922918004572872\n",
      "akasha 6.922918004572872\n",
      "akin 4.9770078555175585\n",
      "alert 6.922918004572872\n",
      "alexander 6.517452896464707\n",
      "alike 6.922918004572872\n",
      "allison 6.922918004572872\n",
      "allow 6.229770824012927\n",
      "allowing 6.922918004572872\n",
      "almost 5.3134800921387715\n",
      "along 6.006627272698717\n",
      "alongside 6.922918004572872\n",
      "already 6.006627272698717\n",
      "also 4.397189360264616\n",
      "although 6.229770824012927\n",
      "always 6.006627272698717\n",
      "amateurish 6.922918004572872\n",
      "amaze 6.517452896464707\n",
      "amazed 6.922918004572872\n",
      "amazing 5.536623643452981\n",
      "amazingly 6.922918004572872\n",
      "america 5.670155036077504\n",
      "american 6.006627272698717\n",
      "americans 6.517452896464707\n",
      "among 6.517452896464707\n",
      "amount 5.824305715904762\n",
      "amusing 6.922918004572872\n",
      "amust 6.922918004572872\n",
      "anatomist 6.922918004572872\n",
      "angel 5.824305715904762\n",
      "angela 6.922918004572872\n",
      "angeles 6.517452896464707\n",
      "angelina 6.922918004572872\n",
      "angle 6.229770824012927\n",
      "angles 6.517452896464707\n",
      "angry 6.922918004572872\n",
      "anguish 6.922918004572872\n",
      "angus 6.922918004572872\n",
      "animals 6.922918004572872\n",
      "animated 6.922918004572872\n",
      "animation 6.517452896464707\n",
      "anita 6.922918004572872\n",
      "ann 4.843476462893037\n",
      "anne 6.006627272698717\n",
      "anniversary 6.922918004572872\n",
      "annoying 5.670155036077504\n",
      "another 5.824305715904762\n",
      "anthony 6.922918004572872\n",
      "antithesis 6.922918004572872\n",
      "anyone 4.843476462893037\n",
      "anything 5.418840607796598\n",
      "anyway 6.922918004572872\n",
      "apart 6.922918004572872\n",
      "appalling 6.517452896464707\n",
      "appealing 6.517452896464707\n",
      "appearance 6.229770824012927\n",
      "appears 6.922918004572872\n",
      "applauded 6.922918004572872\n",
      "applause 6.922918004572872\n",
      "appreciate 6.006627272698717\n",
      "appropriate 6.517452896464707\n",
      "apt 5.824305715904762\n",
      "argued 6.922918004572872\n",
      "armageddon 6.922918004572872\n",
      "armand 6.922918004572872\n",
      "around 5.824305715904762\n",
      "array 6.922918004572872\n",
      "art 3.5217206229107165\n",
      "articulated 6.922918004572872\n",
      "artiness 6.922918004572872\n",
      "artist 6.229770824012927\n",
      "artistic 6.922918004572872\n",
      "artless 6.922918004572872\n",
      "arts 5.418840607796598\n",
      "aside 6.517452896464707\n",
      "ask 6.006627272698717\n",
      "asleep 6.517452896464707\n",
      "aspect 6.229770824012927\n",
      "aspects 6.922918004572872\n",
      "ass 4.480570969203668\n",
      "assante 6.922918004572872\n",
      "assaulted 6.922918004572872\n",
      "assistant 6.922918004572872\n",
      "astonishingly 6.922918004572872\n",
      "astronaut 6.922918004572872\n",
      "atmosphere 6.922918004572872\n",
      "atrocious 6.922918004572872\n",
      "atrocity 6.922918004572872\n",
      "attempt 5.418840607796598\n",
      "attempted 6.922918004572872\n",
      "attempting 6.922918004572872\n",
      "attempts 6.517452896464707\n",
      "attention 6.517452896464707\n",
      "attractive 6.922918004572872\n",
      "audience 5.824305715904762\n",
      "audio 6.922918004572872\n",
      "aurv 6.922918004572872\n",
      "austen 6.922918004572872\n",
      "austere 6.922918004572872\n",
      "author 6.922918004572872\n",
      "average 6.229770824012927\n",
      "aversion 6.922918004572872\n",
      "avoid 5.670155036077504\n",
      "avoided 6.922918004572872\n",
      "award 6.229770824012927\n",
      "awarded 6.922918004572872\n",
      "awards 6.922918004572872\n",
      "away 5.824305715904762\n",
      "awesome 6.229770824012927\n",
      "awful 4.9770078555175585\n",
      "awkwardly 6.922918004572872\n",
      "aye 4.725693427236653\n",
      "baaaaaad 6.922918004572872\n",
      "babbling 6.922918004572872\n",
      "babie 6.922918004572872\n",
      "baby 6.517452896464707\n",
      "babysitting 6.922918004572872\n",
      "back 5.418840607796598\n",
      "backdrop 6.922918004572872\n",
      "backed 6.922918004572872\n",
      "bad 3.573013917298267\n",
      "badly 6.517452896464707\n",
      "bag 5.418840607796598\n",
      "bailey 6.922918004572872\n",
      "bakery 6.922918004572872\n",
      "balance 6.006627272698717\n",
      "balanced 6.517452896464707\n",
      "ball 6.006627272698717\n",
      "ballet 6.922918004572872\n",
      "balls 6.922918004572872\n",
      "band 6.517452896464707\n",
      "barcelona 6.922918004572872\n",
      "barely 6.006627272698717\n",
      "barking 6.922918004572872\n",
      "barney 6.922918004572872\n",
      "barren 6.922918004572872\n",
      "based 6.922918004572872\n",
      "basic 6.229770824012927\n",
      "basically 6.517452896464707\n",
      "bat 6.006627272698717\n",
      "bates 6.922918004572872\n",
      "baxendale 6.922918004572872\n",
      "bear 6.006627272698717\n",
      "beautiful 5.05111582767128\n",
      "beautifully 6.517452896464707\n",
      "bec 5.3134800921387715\n",
      "became 6.517452896464707\n",
      "bechard 6.922918004572872\n",
      "become 5.824305715904762\n",
      "becomes 6.922918004572872\n",
      "began 6.517452896464707\n",
      "begin 5.670155036077504\n",
      "beginning 6.006627272698717\n",
      "behind 6.922918004572872\n",
      "behold 6.922918004572872\n",
      "bela 6.922918004572872\n",
      "believable 5.218169912334447\n",
      "believe 5.3134800921387715\n",
      "believed 6.922918004572872\n",
      "bell 6.229770824012927\n",
      "bellucci 6.922918004572872\n",
      "belly 6.922918004572872\n",
      "belmondo 6.922918004572872\n",
      "ben 6.229770824012927\n",
      "bendingly 6.922918004572872\n",
      "bennett 6.922918004572872\n",
      "bergen 6.922918004572872\n",
      "bertolucci 6.922918004572872\n",
      "best 4.525022731774501\n",
      "better 4.782851841076601\n",
      "betty 6.517452896464707\n",
      "beware 6.922918004572872\n",
      "beyond 5.824305715904762\n",
      "bible 6.922918004572872\n",
      "big 5.131158535344817\n",
      "biggest 6.922918004572872\n",
      "billy 6.229770824012927\n",
      "biographical 6.922918004572872\n",
      "bipolarity 6.922918004572872\n",
      "bit 5.418840607796598\n",
      "bitchy 6.922918004572872\n",
      "black 5.3134800921387715\n",
      "blah 6.922918004572872\n",
      "blake 6.922918004572872\n",
      "bland 6.517452896464707\n",
      "blandly 6.922918004572872\n",
      "blare 6.922918004572872\n",
      "blatant 6.922918004572872\n",
      "blew 6.922918004572872\n",
      "blood 6.517452896464707\n",
      "blown 6.922918004572872\n",
      "blue 6.922918004572872\n",
      "blush 6.922918004572872\n",
      "boasts 6.922918004572872\n",
      "bob 6.922918004572872\n",
      "body 5.824305715904762\n",
      "bohemian 6.922918004572872\n",
      "boiling 6.922918004572872\n",
      "bold 6.517452896464707\n",
      "bombardments 6.922918004572872\n",
      "bond 6.517452896464707\n",
      "bonding 6.922918004572872\n",
      "bonus 6.517452896464707\n",
      "bonuses 6.922918004572872\n",
      "boobs 6.922918004572872\n",
      "boogeyman 6.922918004572872\n",
      "book 6.006627272698717\n",
      "boost 6.922918004572872\n",
      "bop 6.922918004572872\n",
      "bordered 6.922918004572872\n",
      "borderlines 6.922918004572872\n",
      "borders 6.922918004572872\n",
      "bore 5.536623643452981\n",
      "bored 6.006627272698717\n",
      "boring 5.3134800921387715\n",
      "borrowed 6.922918004572872\n",
      "boss 6.922918004572872\n",
      "bother 6.229770824012927\n",
      "bothersome 6.922918004572872\n",
      "bought 6.229770824012927\n",
      "box 6.922918004572872\n",
      "boyfriend 6.922918004572872\n",
      "boyle 6.922918004572872\n",
      "brain 6.229770824012927\n",
      "brainsucking 6.922918004572872\n",
      "brat 6.517452896464707\n",
      "breaking 6.922918004572872\n",
      "breeders 6.922918004572872\n",
      "brevity 6.922918004572872\n",
      "brian 6.517452896464707\n",
      "brief 6.517452896464707\n",
      "brigand 6.922918004572872\n",
      "bright 6.922918004572872\n",
      "brilliance 6.229770824012927\n",
      "brilliant 5.536623643452981\n",
      "brilliantly 6.922918004572872\n",
      "bring 6.229770824012927\n",
      "brings 6.922918004572872\n",
      "broad 6.517452896464707\n",
      "broke 6.922918004572872\n",
      "brooding 6.922918004572872\n",
      "brother 6.922918004572872\n",
      "brutal 6.922918004572872\n",
      "buddy 6.922918004572872\n",
      "budget 5.536623643452981\n",
      "buffalo 6.922918004572872\n",
      "buffet 6.922918004572872\n",
      "build 6.229770824012927\n",
      "builders 6.922918004572872\n",
      "buildings 6.922918004572872\n",
      "built 6.922918004572872\n",
      "bullock 6.517452896464707\n",
      "bully 6.922918004572872\n",
      "bunch 6.517452896464707\n",
      "burton 6.922918004572872\n",
      "business 6.517452896464707\n",
      "buy 6.517452896464707\n",
      "cable 6.517452896464707\n",
      "cailles 6.922918004572872\n",
      "california 6.922918004572872\n",
      "call 4.9770078555175585\n",
      "called 6.006627272698717\n",
      "calls 6.922918004572872\n",
      "came 4.782851841076601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cameo 6.922918004572872\n",
      "camera 5.131158535344817\n",
      "camerawork 6.922918004572872\n",
      "camp 6.229770824012927\n",
      "campy 6.922918004572872\n",
      "canada 6.517452896464707\n",
      "cancan 6.922918004572872\n",
      "candace 6.922918004572872\n",
      "candle 6.922918004572872\n",
      "cannot 6.517452896464707\n",
      "cant 6.229770824012927\n",
      "captain 6.922918004572872\n",
      "captured 6.922918004572872\n",
      "captures 6.922918004572872\n",
      "car 4.214867803470662\n",
      "card 6.006627272698717\n",
      "cardboard 6.517452896464707\n",
      "cardellini 6.922918004572872\n",
      "care 5.218169912334447\n",
      "carol 6.922918004572872\n",
      "carrell 6.922918004572872\n",
      "carries 6.922918004572872\n",
      "carry 6.922918004572872\n",
      "cars 6.922918004572872\n",
      "cartoon 5.824305715904762\n",
      "cartoons 6.517452896464707\n",
      "case 6.229770824012927\n",
      "cases 6.922918004572872\n",
      "cast 4.397189360264616\n",
      "casted 6.922918004572872\n",
      "casting 5.670155036077504\n",
      "cat 5.3134800921387715\n",
      "catchy 6.922918004572872\n",
      "caught 6.922918004572872\n",
      "cause 6.517452896464707\n",
      "ceases 6.517452896464707\n",
      "celebration 6.922918004572872\n",
      "celebrity 6.922918004572872\n",
      "celluloid 6.922918004572872\n",
      "centers 6.922918004572872\n",
      "central 6.517452896464707\n",
      "century 6.517452896464707\n",
      "certain 5.418840607796598\n",
      "certainly 5.536623643452981\n",
      "cg 6.517452896464707\n",
      "cgi 6.922918004572872\n",
      "chalkboard 6.922918004572872\n",
      "challenges 6.922918004572872\n",
      "chance 6.517452896464707\n",
      "change 6.006627272698717\n",
      "changes 6.922918004572872\n",
      "changing 6.922918004572872\n",
      "channel 6.922918004572872\n",
      "character 3.7874237886437223\n",
      "characterisation 6.922918004572872\n",
      "characters 4.150329282333091\n",
      "charisma 6.229770824012927\n",
      "charismatic 6.922918004572872\n",
      "charles 6.517452896464707\n",
      "charlie 6.922918004572872\n",
      "charm 5.824305715904762\n",
      "charming 6.229770824012927\n",
      "chase 6.922918004572872\n",
      "chasing 6.922918004572872\n",
      "cheap 5.418840607796598\n",
      "cheaply 6.922918004572872\n",
      "check 6.006627272698717\n",
      "checking 6.229770824012927\n",
      "cheek 6.517452896464707\n",
      "cheekbones 6.922918004572872\n",
      "cheerfull 6.922918004572872\n",
      "cheerless 6.922918004572872\n",
      "cheesiness 6.922918004572872\n",
      "cheesy 6.517452896464707\n",
      "chemistry 6.006627272698717\n",
      "chick 6.517452896464707\n",
      "child 5.131158535344817\n",
      "childhood 6.517452896464707\n",
      "children 6.006627272698717\n",
      "childrens 6.922918004572872\n",
      "chills 6.922918004572872\n",
      "chilly 6.922918004572872\n",
      "chimp 6.922918004572872\n",
      "chodorov 6.922918004572872\n",
      "choice 6.922918004572872\n",
      "choices 6.922918004572872\n",
      "choked 6.922918004572872\n",
      "chosen 6.922918004572872\n",
      "chow 6.922918004572872\n",
      "christmas 6.517452896464707\n",
      "christopher 6.922918004572872\n",
      "church 6.517452896464707\n",
      "cinema 4.725693427236653\n",
      "cinematic 6.922918004572872\n",
      "cinematographers 6.922918004572872\n",
      "cinematography 5.3134800921387715\n",
      "circumstances 6.922918004572872\n",
      "class 5.418840607796598\n",
      "classic 5.536623643452981\n",
      "classical 6.922918004572872\n",
      "clear 6.229770824012927\n",
      "clearly 6.517452896464707\n",
      "clever 5.824305715904762\n",
      "clich 6.006627272698717\n",
      "cliche 6.922918004572872\n",
      "clients 6.922918004572872\n",
      "cliff 6.922918004572872\n",
      "climax 6.922918004572872\n",
      "close 6.006627272698717\n",
      "closed 6.922918004572872\n",
      "clothes 6.922918004572872\n",
      "club 6.922918004572872\n",
      "co 2.406579032291396\n",
      "coach 6.922918004572872\n",
      "coal 6.922918004572872\n",
      "coastal 6.922918004572872\n",
      "coaster 6.922918004572872\n",
      "coherent 6.922918004572872\n",
      "cold 6.922918004572872\n",
      "cole 6.517452896464707\n",
      "collect 6.517452896464707\n",
      "collective 6.922918004572872\n",
      "colored 6.922918004572872\n",
      "colorful 6.922918004572872\n",
      "colours 6.922918004572872\n",
      "columbo 6.922918004572872\n",
      "come 4.480570969203668\n",
      "comedic 6.922918004572872\n",
      "comedy 5.536623643452981\n",
      "comes 5.824305715904762\n",
      "comfortable 6.922918004572872\n",
      "comforting 6.922918004572872\n",
      "comical 6.922918004572872\n",
      "coming 6.229770824012927\n",
      "commands 6.922918004572872\n",
      "comment 6.006627272698717\n",
      "commentary 6.922918004572872\n",
      "commented 6.922918004572872\n",
      "comments 6.922918004572872\n",
      "commercial 6.922918004572872\n",
      "community 6.922918004572872\n",
      "company 6.922918004572872\n",
      "compelling 6.517452896464707\n",
      "competent 6.922918004572872\n",
      "complete 5.418840607796598\n",
      "completed 6.922918004572872\n",
      "completely 6.006627272698717\n",
      "complex 6.229770824012927\n",
      "complexity 6.922918004572872\n",
      "composed 6.922918004572872\n",
      "composition 6.517452896464707\n",
      "comprehensible 6.517452896464707\n",
      "compromise 6.922918004572872\n",
      "computer 6.517452896464707\n",
      "concentrate 6.922918004572872\n",
      "conception 6.922918004572872\n",
      "conceptually 6.922918004572872\n",
      "concerning 6.922918004572872\n",
      "concerns 6.922918004572872\n",
      "concert 6.922918004572872\n",
      "conclusion 6.229770824012927\n",
      "condescends 6.922918004572872\n",
      "confidence 6.922918004572872\n",
      "configuration 6.922918004572872\n",
      "confirm 6.922918004572872\n",
      "conflict 6.229770824012927\n",
      "confuses 6.922918004572872\n",
      "confusing 6.922918004572872\n",
      "connections 6.922918004572872\n",
      "connery 6.517452896464707\n",
      "connor 6.922918004572872\n",
      "conrad 6.922918004572872\n",
      "consequences 6.922918004572872\n",
      "consider 5.418840607796598\n",
      "considerable 6.922918004572872\n",
      "considered 6.922918004572872\n",
      "considering 6.517452896464707\n",
      "considers 6.922918004572872\n",
      "consistent 6.922918004572872\n",
      "consolations 6.922918004572872\n",
      "constant 6.517452896464707\n",
      "constantine 6.922918004572872\n",
      "constructed 6.922918004572872\n",
      "contained 6.517452896464707\n",
      "containing 6.922918004572872\n",
      "contains 6.517452896464707\n",
      "content 6.922918004572872\n",
      "continually 6.922918004572872\n",
      "continuation 6.922918004572872\n",
      "continue 6.922918004572872\n",
      "continuity 6.517452896464707\n",
      "continuously 6.922918004572872\n",
      "contract 6.922918004572872\n",
      "contrast 6.517452896464707\n",
      "contributing 6.922918004572872\n",
      "contributory 6.922918004572872\n",
      "contrived 6.922918004572872\n",
      "control 6.517452896464707\n",
      "controversy 6.922918004572872\n",
      "convention 6.922918004572872\n",
      "convey 6.922918004572872\n",
      "convince 6.922918004572872\n",
      "convincing 5.536623643452981\n",
      "convoluted 6.922918004572872\n",
      "cool 5.536623643452981\n",
      "coppola 6.517452896464707\n",
      "cords 6.922918004572872\n",
      "core 5.824305715904762\n",
      "corn 6.229770824012927\n",
      "corny 6.922918004572872\n",
      "correct 6.517452896464707\n",
      "cost 5.670155036077504\n",
      "costs 6.229770824012927\n",
      "costumes 6.922918004572872\n",
      "cotton 6.922918004572872\n",
      "could 4.397189360264616\n",
      "couple 6.922918004572872\n",
      "course 6.229770824012927\n",
      "court 6.229770824012927\n",
      "courtroom 6.517452896464707\n",
      "cover 5.418840607796598\n",
      "cowardice 6.922918004572872\n",
      "cox 6.922918004572872\n",
      "crackles 6.922918004572872\n",
      "crafted 6.517452896464707\n",
      "crap 5.824305715904762\n",
      "crash 6.517452896464707\n",
      "crashed 6.922918004572872\n",
      "crayon 6.517452896464707\n",
      "crayons 6.922918004572872\n",
      "crazy 6.517452896464707\n",
      "create 5.418840607796598\n",
      "created 6.006627272698717\n",
      "creates 6.517452896464707\n",
      "creative 6.922918004572872\n",
      "creativity 6.922918004572872\n",
      "creature 6.922918004572872\n",
      "credible 5.824305715904762\n",
      "credit 6.229770824012927\n",
      "credits 6.922918004572872\n",
      "crew 6.922918004572872\n",
      "crime 6.922918004572872\n",
      "crisp 6.922918004572872\n",
      "critic 6.517452896464707\n",
      "critical 6.922918004572872\n",
      "crocdodile 6.922918004572872\n",
      "crocs 6.922918004572872\n",
      "cross 6.922918004572872\n",
      "crowd 6.517452896464707\n",
      "crowe 6.922918004572872\n",
      "cruel 6.922918004572872\n",
      "cruise 6.922918004572872\n",
      "cry 6.922918004572872\n",
      "cult 5.670155036077504\n",
      "culture 6.517452896464707\n",
      "curtain 6.922918004572872\n",
      "custer 6.922918004572872\n",
      "cute 6.006627272698717\n",
      "cutest 6.922918004572872\n",
      "cutie 6.922918004572872\n",
      "cutouts 6.922918004572872\n",
      "cuts 6.922918004572872\n",
      "cutting 6.922918004572872\n",
      "dads 6.922918004572872\n",
      "damian 6.922918004572872\n",
      "damn 6.922918004572872\n",
      "dance 6.006627272698717\n",
      "dancing 6.517452896464707\n",
      "dangerous 6.517452896464707\n",
      "dark 6.517452896464707\n",
      "darren 6.922918004572872\n",
      "daughter 6.229770824012927\n",
      "daughters 6.922918004572872\n",
      "day 5.131158535344817\n",
      "days 6.517452896464707\n",
      "de 2.406579032291396\n",
      "dead 6.006627272698717\n",
      "deadly 6.922918004572872\n",
      "deadpan 6.922918004572872\n",
      "deal 6.006627272698717\n",
      "dealt 6.922918004572872\n",
      "death 6.006627272698717\n",
      "debated 6.922918004572872\n",
      "debbie 6.922918004572872\n",
      "debits 6.922918004572872\n",
      "debut 6.922918004572872\n",
      "decay 6.517452896464707\n",
      "decent 6.229770824012927\n",
      "decidely 6.922918004572872\n",
      "decipher 6.922918004572872\n",
      "decisions 6.922918004572872\n",
      "dedication 6.517452896464707\n",
      "dee 5.218169912334447\n",
      "deep 6.229770824012927\n",
      "deeply 6.517452896464707\n",
      "defensemen 6.922918004572872\n",
      "defined 6.922918004572872\n",
      "definitely 5.3134800921387715\n",
      "delete 6.922918004572872\n",
      "delight 6.006627272698717\n",
      "delightful 6.922918004572872\n",
      "delights 6.922918004572872\n",
      "deliver 5.536623643452981\n",
      "delivered 6.922918004572872\n",
      "delivering 6.517452896464707\n",
      "delivers 6.229770824012927\n",
      "dependant 6.922918004572872\n",
      "depending 6.922918004572872\n",
      "depends 6.922918004572872\n",
      "depicted 6.922918004572872\n",
      "depicts 6.517452896464707\n",
      "depressing 6.517452896464707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depth 6.006627272698717\n",
      "derivative 6.922918004572872\n",
      "describe 6.229770824012927\n",
      "describes 6.922918004572872\n",
      "desert 6.922918004572872\n",
      "deserved 6.517452896464707\n",
      "deserves 6.517452896464707\n",
      "deserving 6.517452896464707\n",
      "design 6.229770824012927\n",
      "designed 6.922918004572872\n",
      "designer 6.922918004572872\n",
      "desperately 6.922918004572872\n",
      "desperation 6.922918004572872\n",
      "despised 6.922918004572872\n",
      "despite 6.517452896464707\n",
      "destroy 6.922918004572872\n",
      "detailing 6.922918004572872\n",
      "details 6.922918004572872\n",
      "develop 6.517452896464707\n",
      "development 6.517452896464707\n",
      "developments 6.922918004572872\n",
      "di 2.681591252002126\n",
      "diabetic 6.922918004572872\n",
      "dialog 4.908014984030608\n",
      "dialogs 6.922918004572872\n",
      "dialogue 5.131158535344817\n",
      "diaper 6.922918004572872\n",
      "dickens 6.922918004572872\n",
      "difference 6.922918004572872\n",
      "different 5.824305715904762\n",
      "dignity 6.922918004572872\n",
      "dimensional 6.922918004572872\n",
      "direct 4.438011354784871\n",
      "directed 6.229770824012927\n",
      "directing 5.418840607796598\n",
      "direction 6.006627272698717\n",
      "director 5.131158535344817\n",
      "directorial 6.517452896464707\n",
      "directors 6.922918004572872\n",
      "disappointed 5.670155036077504\n",
      "disappointing 6.517452896464707\n",
      "disappointment 6.517452896464707\n",
      "disaster 6.517452896464707\n",
      "disbelief 6.922918004572872\n",
      "discomfort 6.922918004572872\n",
      "discovering 6.517452896464707\n",
      "discovery 6.922918004572872\n",
      "disgrace 6.922918004572872\n",
      "disgusting 6.922918004572872\n",
      "dislike 6.006627272698717\n",
      "disliked 6.517452896464707\n",
      "disney 6.922918004572872\n",
      "disparate 6.922918004572872\n",
      "distant 6.922918004572872\n",
      "distinction 6.922918004572872\n",
      "distorted 6.922918004572872\n",
      "distract 6.922918004572872\n",
      "distressed 6.922918004572872\n",
      "disturbing 6.517452896464707\n",
      "diving 6.922918004572872\n",
      "doctor 6.517452896464707\n",
      "documentaries 6.517452896464707\n",
      "documentary 6.229770824012927\n",
      "dodge 6.922918004572872\n",
      "dogs 6.517452896464707\n",
      "dollars 6.922918004572872\n",
      "dominated 6.922918004572872\n",
      "done 5.536623643452981\n",
      "donlevy 6.922918004572872\n",
      "dont 6.922918004572872\n",
      "doomed 6.922918004572872\n",
      "dose 6.922918004572872\n",
      "doubt 6.229770824012927\n",
      "downs 6.922918004572872\n",
      "dozen 6.922918004572872\n",
      "dr 4.0325462466767075\n",
      "dracula 6.922918004572872\n",
      "draft 6.922918004572872\n",
      "drag 6.006627272698717\n",
      "drago 6.229770824012927\n",
      "drama 5.131158535344817\n",
      "dramatic 6.517452896464707\n",
      "drawings 6.922918004572872\n",
      "drawn 6.922918004572872\n",
      "dream 6.229770824012927\n",
      "dreams 6.517452896464707\n",
      "dreary 6.922918004572872\n",
      "dribble 6.922918004572872\n",
      "drift 6.229770824012927\n",
      "drifting 6.922918004572872\n",
      "drive 6.229770824012927\n",
      "drooling 6.922918004572872\n",
      "dropped 6.922918004572872\n",
      "dry 6.922918004572872\n",
      "due 5.670155036077504\n",
      "duet 6.517452896464707\n",
      "dull 6.517452896464707\n",
      "dumb 6.229770824012927\n",
      "dumbest 6.922918004572872\n",
      "duper 6.922918004572872\n",
      "duris 6.922918004572872\n",
      "dustin 6.922918004572872\n",
      "dvd 6.229770824012927\n",
      "dwight 6.922918004572872\n",
      "dysfunction 6.922918004572872\n",
      "earlier 6.229770824012927\n",
      "early 5.670155036077504\n",
      "earth 6.517452896464707\n",
      "easily 6.229770824012927\n",
      "easy 6.006627272698717\n",
      "eating 6.517452896464707\n",
      "ebay 6.922918004572872\n",
      "ebola 6.922918004572872\n",
      "eccleston 6.922918004572872\n",
      "ed 2.0515447778101237\n",
      "edge 6.229770824012927\n",
      "editing 5.824305715904762\n",
      "edition 6.922918004572872\n",
      "educational 6.922918004572872\n",
      "edward 6.922918004572872\n",
      "effect 5.131158535344817\n",
      "effective 6.229770824012927\n",
      "effects 5.418840607796598\n",
      "effort 6.517452896464707\n",
      "efforts 6.922918004572872\n",
      "egotism 6.922918004572872\n",
      "eighth 6.922918004572872\n",
      "eiko 6.922918004572872\n",
      "either 6.229770824012927\n",
      "elaborately 6.922918004572872\n",
      "elderly 6.922918004572872\n",
      "elegant 6.922918004572872\n",
      "element 6.517452896464707\n",
      "elias 6.922918004572872\n",
      "eloquently 6.922918004572872\n",
      "else 5.824305715904762\n",
      "elsewhere 6.922918004572872\n",
      "embarrassed 6.922918004572872\n",
      "embarrassing 6.229770824012927\n",
      "embassy 6.922918004572872\n",
      "emerge 6.922918004572872\n",
      "emilio 6.517452896464707\n",
      "emily 6.517452896464707\n",
      "emoting 6.922918004572872\n",
      "emotion 6.006627272698717\n",
      "emotionally 6.922918004572872\n",
      "emotions 6.517452896464707\n",
      "emperor 6.922918004572872\n",
      "empowerment 6.922918004572872\n",
      "emptiness 6.922918004572872\n",
      "empty 6.517452896464707\n",
      "en 1.88921743754562\n",
      "enchanting 6.922918004572872\n",
      "end 3.505191320959506\n",
      "endearing 6.922918004572872\n",
      "ended 5.418840607796598\n",
      "ending 5.05111582767128\n",
      "endlessly 6.922918004572872\n",
      "ends 5.670155036077504\n",
      "energetic 6.922918004572872\n",
      "energy 6.006627272698717\n",
      "engaging 6.517452896464707\n",
      "english 6.922918004572872\n",
      "enhanced 6.922918004572872\n",
      "enjoy 4.843476462893037\n",
      "enjoyable 6.229770824012927\n",
      "enjoyed 5.418840607796598\n",
      "enjoyment 6.922918004572872\n",
      "enough 5.218169912334447\n",
      "enter 5.418840607796598\n",
      "enterprise 6.922918004572872\n",
      "entertained 6.922918004572872\n",
      "entertaining 6.006627272698717\n",
      "entire 5.418840607796598\n",
      "entirely 6.517452896464707\n",
      "entrance 6.922918004572872\n",
      "episode 6.006627272698717\n",
      "episodes 6.922918004572872\n",
      "equivalent 6.922918004572872\n",
      "era 3.854865069439255\n",
      "errol 6.517452896464707\n",
      "errors 6.922918004572872\n",
      "escalating 6.922918004572872\n",
      "escapism 6.922918004572872\n",
      "especially 5.418840607796598\n",
      "essence 6.922918004572872\n",
      "establish 6.517452896464707\n",
      "established 6.922918004572872\n",
      "estate 6.922918004572872\n",
      "estevez 6.922918004572872\n",
      "etc 6.229770824012927\n",
      "european 6.517452896464707\n",
      "evaluate 6.922918004572872\n",
      "even 3.9784790254064317\n",
      "events 6.922918004572872\n",
      "ever 3.0727704028628136\n",
      "every 4.089704660516656\n",
      "everybody 6.922918004572872\n",
      "everyone 5.418840607796598\n",
      "everything 5.05111582767128\n",
      "everywhere 6.922918004572872\n",
      "evidently 6.922918004572872\n",
      "evil 6.517452896464707\n",
      "evinced 6.922918004572872\n",
      "evokes 6.922918004572872\n",
      "exactly 6.229770824012927\n",
      "exaggerating 6.922918004572872\n",
      "example 6.229770824012927\n",
      "excellent 4.843476462893037\n",
      "excellently 6.517452896464707\n",
      "except 6.006627272698717\n",
      "exceptional 6.229770824012927\n",
      "exceptionally 6.922918004572872\n",
      "excerpts 6.922918004572872\n",
      "excessively 6.922918004572872\n",
      "exchange 6.922918004572872\n",
      "exciting 6.922918004572872\n",
      "excruciatingly 6.922918004572872\n",
      "excuse 6.517452896464707\n",
      "excuses 6.922918004572872\n",
      "executed 6.922918004572872\n",
      "exemplars 6.922918004572872\n",
      "existent 6.517452896464707\n",
      "existential 6.922918004572872\n",
      "expansive 6.922918004572872\n",
      "expect 5.536623643452981\n",
      "expectations 6.922918004572872\n",
      "expected 6.922918004572872\n",
      "expecting 6.922918004572872\n",
      "experience 5.418840607796598\n",
      "experiences 6.922918004572872\n",
      "expert 6.922918004572872\n",
      "explain 6.229770824012927\n",
      "explains 6.922918004572872\n",
      "explanation 6.517452896464707\n",
      "exploit 6.922918004572872\n",
      "explorations 6.922918004572872\n",
      "explosion 6.922918004572872\n",
      "expression 6.922918004572872\n",
      "exquisite 6.517452896464707\n",
      "extant 6.922918004572872\n",
      "exteriors 6.922918004572872\n",
      "extraneous 6.922918004572872\n",
      "extraordinary 6.922918004572872\n",
      "extremely 6.517452896464707\n",
      "eye 5.418840607796598\n",
      "eyes 6.006627272698717\n",
      "fabulous 6.922918004572872\n",
      "face 5.670155036077504\n",
      "faces 6.922918004572872\n",
      "facial 6.922918004572872\n",
      "facing 6.922918004572872\n",
      "fact 5.218169912334447\n",
      "factory 6.517452896464707\n",
      "failed 6.922918004572872\n",
      "fails 5.824305715904762\n",
      "fair 6.517452896464707\n",
      "fairly 6.922918004572872\n",
      "faithful 6.517452896464707\n",
      "fall 6.006627272698717\n",
      "falling 6.922918004572872\n",
      "falls 6.922918004572872\n",
      "falsely 6.922918004572872\n",
      "falwell 6.922918004572872\n",
      "fame 6.517452896464707\n",
      "famed 6.922918004572872\n",
      "family 5.536623643452981\n",
      "famous 6.922918004572872\n",
      "fan 5.05111582767128\n",
      "fanciful 6.922918004572872\n",
      "fans 5.824305715904762\n",
      "fantastic 6.229770824012927\n",
      "fantasy 6.922918004572872\n",
      "far 5.418840607796598\n",
      "farce 6.922918004572872\n",
      "fare 6.922918004572872\n",
      "fascinated 6.922918004572872\n",
      "fascinating 6.517452896464707\n",
      "fascination 6.922918004572872\n",
      "fashioned 6.922918004572872\n",
      "fast 5.824305715904762\n",
      "faster 6.922918004572872\n",
      "fat 6.229770824012927\n",
      "father 6.922918004572872\n",
      "faultless 6.922918004572872\n",
      "fausa 6.922918004572872\n",
      "faux 6.229770824012927\n",
      "favorite 6.922918004572872\n",
      "favourite 6.229770824012927\n",
      "fear 6.229770824012927\n",
      "feature 6.006627272698717\n",
      "features 6.229770824012927\n",
      "feel 5.05111582767128\n",
      "feeling 5.3134800921387715\n",
      "feelings 6.922918004572872\n",
      "feet 6.922918004572872\n",
      "feisty 6.922918004572872\n",
      "fellowes 6.922918004572872\n",
      "felt 5.824305715904762\n",
      "female 6.229770824012927\n",
      "females 6.922918004572872\n",
      "ferry 6.922918004572872\n",
      "fest 6.922918004572872\n",
      "fi 2.369041112972331\n",
      "fields 6.922918004572872\n",
      "fifteen 6.922918004572872\n",
      "fifties 6.922918004572872\n",
      "fill 6.517452896464707\n",
      "film 2.6532205548729104\n",
      "filmed 6.517452896464707\n",
      "filmiing 6.922918004572872\n",
      "filmmaker 6.922918004572872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filmography 6.922918004572872\n",
      "films 4.671626205966376\n",
      "final 5.536623643452981\n",
      "finale 6.922918004572872\n",
      "finally 6.229770824012927\n",
      "financial 6.922918004572872\n",
      "find 5.131158535344817\n",
      "finds 6.922918004572872\n",
      "fine 5.536623643452981\n",
      "finest 6.922918004572872\n",
      "fingernails 6.922918004572872\n",
      "finished 6.922918004572872\n",
      "fire 6.922918004572872\n",
      "first 5.218169912334447\n",
      "fish 6.517452896464707\n",
      "fishnet 6.922918004572872\n",
      "fisted 6.922918004572872\n",
      "fit 6.922918004572872\n",
      "five 6.517452896464707\n",
      "flag 6.922918004572872\n",
      "flakes 6.922918004572872\n",
      "flaming 6.922918004572872\n",
      "flashbacks 6.922918004572872\n",
      "flat 6.517452896464707\n",
      "flaw 5.824305715904762\n",
      "flawed 6.517452896464707\n",
      "flaws 6.517452896464707\n",
      "fleshed 6.922918004572872\n",
      "flick 5.824305715904762\n",
      "flicks 6.922918004572872\n",
      "florida 6.922918004572872\n",
      "flowed 6.922918004572872\n",
      "flying 6.922918004572872\n",
      "flynn 6.517452896464707\n",
      "focus 6.517452896464707\n",
      "fodder 6.922918004572872\n",
      "follow 5.670155036077504\n",
      "following 6.517452896464707\n",
      "follows 6.922918004572872\n",
      "foolish 6.922918004572872\n",
      "footage 6.229770824012927\n",
      "football 6.922918004572872\n",
      "force 6.006627272698717\n",
      "forced 6.922918004572872\n",
      "forces 6.517452896464707\n",
      "ford 6.922918004572872\n",
      "foreign 6.517452896464707\n",
      "foreigner 6.922918004572872\n",
      "forever 6.922918004572872\n",
      "forget 5.824305715904762\n",
      "forgettable 6.517452896464707\n",
      "forgetting 6.922918004572872\n",
      "forgot 6.517452896464707\n",
      "forgotten 6.922918004572872\n",
      "form 4.671626205966376\n",
      "format 6.922918004572872\n",
      "former 6.922918004572872\n",
      "fort 5.05111582767128\n",
      "forth 6.922918004572872\n",
      "forwarded 6.922918004572872\n",
      "found 5.536623643452981\n",
      "four 6.922918004572872\n",
      "fox 6.229770824012927\n",
      "foxx 6.922918004572872\n",
      "frances 6.922918004572872\n",
      "francis 6.922918004572872\n",
      "frankly 6.517452896464707\n",
      "free 5.670155036077504\n",
      "freedom 6.517452896464707\n",
      "freeman 6.922918004572872\n",
      "french 6.517452896464707\n",
      "fresh 6.006627272698717\n",
      "freshness 6.922918004572872\n",
      "friends 6.229770824012927\n",
      "friendship 6.922918004572872\n",
      "frightening 6.229770824012927\n",
      "front 6.229770824012927\n",
      "frontier 6.922918004572872\n",
      "frost 6.922918004572872\n",
      "frustration 6.922918004572872\n",
      "fulci 6.517452896464707\n",
      "fulfilling 6.922918004572872\n",
      "full 4.908014984030608\n",
      "fully 5.418840607796598\n",
      "fumbling 6.922918004572872\n",
      "fun 4.438011354784871\n",
      "function 6.517452896464707\n",
      "fundamental 6.922918004572872\n",
      "funniest 6.922918004572872\n",
      "funny 4.725693427236653\n",
      "future 6.517452896464707\n",
      "fx 6.517452896464707\n",
      "gabriel 6.922918004572872\n",
      "gadget 6.922918004572872\n",
      "gain 6.922918004572872\n",
      "gake 6.922918004572872\n",
      "galley 6.922918004572872\n",
      "gallon 6.922918004572872\n",
      "game 5.670155036077504\n",
      "games 6.229770824012927\n",
      "garage 6.922918004572872\n",
      "garbage 5.536623643452981\n",
      "garbo 6.922918004572872\n",
      "garfield 6.922918004572872\n",
      "gas 6.922918004572872\n",
      "gaudi 6.922918004572872\n",
      "gave 5.824305715904762\n",
      "gay 6.922918004572872\n",
      "geek 6.922918004572872\n",
      "gem 6.229770824012927\n",
      "general 5.824305715904762\n",
      "generally 6.006627272698717\n",
      "generates 6.922918004572872\n",
      "generic 6.922918004572872\n",
      "genius 6.517452896464707\n",
      "genre 6.517452896464707\n",
      "gently 6.922918004572872\n",
      "genuine 6.229770824012927\n",
      "george 6.922918004572872\n",
      "gerardo 6.922918004572872\n",
      "gere 6.922918004572872\n",
      "get 3.9784790254064317\n",
      "gets 5.670155036077504\n",
      "getting 6.517452896464707\n",
      "ghibili 6.922918004572872\n",
      "giallo 6.229770824012927\n",
      "gibberish 6.922918004572872\n",
      "gifted 6.922918004572872\n",
      "giovanni 6.922918004572872\n",
      "girl 5.218169912334447\n",
      "girlfriend 6.517452896464707\n",
      "girls 6.922918004572872\n",
      "girolamo 6.922918004572872\n",
      "give 4.725693427236653\n",
      "given 6.229770824012927\n",
      "gives 5.824305715904762\n",
      "giving 6.922918004572872\n",
      "glad 6.229770824012927\n",
      "glance 6.517452896464707\n",
      "glasses 6.922918004572872\n",
      "gloriously 6.922918004572872\n",
      "go 3.105205678615967\n",
      "goalies 6.922918004572872\n",
      "god 6.006627272698717\n",
      "goes 6.517452896464707\n",
      "going 5.218169912334447\n",
      "gone 6.922918004572872\n",
      "gonna 6.517452896464707\n",
      "good 3.7874237886437223\n",
      "gore 6.517452896464707\n",
      "goremeister 6.922918004572872\n",
      "gorman 6.922918004572872\n",
      "gosh 6.922918004572872\n",
      "got 4.908014984030608\n",
      "goth 6.922918004572872\n",
      "gotta 6.922918004572872\n",
      "gotten 6.517452896464707\n",
      "government 6.922918004572872\n",
      "grace 6.006627272698717\n",
      "grade 6.922918004572872\n",
      "gradually 6.922918004572872\n",
      "grainy 6.922918004572872\n",
      "granted 6.922918004572872\n",
      "graphics 6.922918004572872\n",
      "grasp 6.922918004572872\n",
      "grates 6.922918004572872\n",
      "great 4.119557623666337\n",
      "greatest 6.229770824012927\n",
      "greatness 6.922918004572872\n",
      "green 6.517452896464707\n",
      "greenstreet 6.922918004572872\n",
      "grew 6.922918004572872\n",
      "grim 6.517452896464707\n",
      "grimes 6.922918004572872\n",
      "gripping 6.517452896464707\n",
      "groove 6.922918004572872\n",
      "gross 6.922918004572872\n",
      "ground 6.922918004572872\n",
      "guards 6.922918004572872\n",
      "guess 5.824305715904762\n",
      "guests 6.922918004572872\n",
      "guilt 6.922918004572872\n",
      "gung 6.922918004572872\n",
      "guy 6.229770824012927\n",
      "guys 6.922918004572872\n",
      "hackneyed 6.517452896464707\n",
      "haggis 6.922918004572872\n",
      "hair 6.517452896464707\n",
      "hairsplitting 6.922918004572872\n",
      "half 5.536623643452981\n",
      "halfway 6.922918004572872\n",
      "ham 6.006627272698717\n",
      "hand 5.418840607796598\n",
      "handle 6.006627272698717\n",
      "handled 6.517452896464707\n",
      "handles 6.922918004572872\n",
      "hands 6.517452896464707\n",
      "hang 5.670155036077504\n",
      "hankies 6.922918004572872\n",
      "hanks 6.517452896464707\n",
      "happen 5.670155036077504\n",
      "happened 6.229770824012927\n",
      "happiness 6.922918004572872\n",
      "happy 6.922918004572872\n",
      "hard 6.006627272698717\n",
      "harris 6.922918004572872\n",
      "hate 5.218169912334447\n",
      "hated 6.517452896464707\n",
      "hatred 6.922918004572872\n",
      "havilland 6.922918004572872\n",
      "hay 6.229770824012927\n",
      "hayao 6.922918004572872\n",
      "hayworth 6.922918004572872\n",
      "hbo 6.517452896464707\n",
      "head 6.006627272698717\n",
      "heads 6.922918004572872\n",
      "hear 5.3134800921387715\n",
      "heard 6.517452896464707\n",
      "heart 5.824305715904762\n",
      "hearts 6.922918004572872\n",
      "heartwarming 6.922918004572872\n",
      "heaven 6.517452896464707\n",
      "heche 6.922918004572872\n",
      "heels 6.922918004572872\n",
      "heist 6.922918004572872\n",
      "helen 6.922918004572872\n",
      "hell 6.006627272698717\n",
      "hellish 6.922918004572872\n",
      "helms 6.922918004572872\n",
      "help 6.006627272698717\n",
      "helping 6.922918004572872\n",
      "helps 6.517452896464707\n",
      "hence 6.517452896464707\n",
      "hendrikson 6.922918004572872\n",
      "hernandez 6.922918004572872\n",
      "hero 6.006627272698717\n",
      "heroes 6.922918004572872\n",
      "heroine 6.922918004572872\n",
      "heroism 6.922918004572872\n",
      "hes 5.670155036077504\n",
      "hide 6.922918004572872\n",
      "high 4.908014984030608\n",
      "higher 6.922918004572872\n",
      "highest 6.922918004572872\n",
      "highlights 6.922918004572872\n",
      "highly 5.3134800921387715\n",
      "hilarious 5.670155036077504\n",
      "hill 5.670155036077504\n",
      "hilt 6.922918004572872\n",
      "hip 5.824305715904762\n",
      "history 5.670155036077504\n",
      "hitchcock 6.229770824012927\n",
      "ho 2.6532205548729104\n",
      "hockey 6.922918004572872\n",
      "hoffman 6.922918004572872\n",
      "hold 5.824305715904762\n",
      "holding 6.922918004572872\n",
      "holds 6.922918004572872\n",
      "holes 5.670155036077504\n",
      "hollander 6.922918004572872\n",
      "hollow 6.517452896464707\n",
      "hollywood 6.517452896464707\n",
      "home 6.006627272698717\n",
      "homework 6.922918004572872\n",
      "honest 6.517452896464707\n",
      "honestly 6.922918004572872\n",
      "hoot 6.517452896464707\n",
      "hope 5.670155036077504\n",
      "hopefully 6.922918004572872\n",
      "hopeless 6.922918004572872\n",
      "horrendous 6.517452896464707\n",
      "horrendously 6.922918004572872\n",
      "horrible 5.824305715904762\n",
      "horrid 6.922918004572872\n",
      "horrified 6.922918004572872\n",
      "horror 5.418840607796598\n",
      "horse 6.517452896464707\n",
      "hosting 6.922918004572872\n",
      "hot 4.908014984030608\n",
      "hour 5.536623643452981\n",
      "hours 6.229770824012927\n",
      "house 5.824305715904762\n",
      "houses 6.922918004572872\n",
      "howdy 6.922918004572872\n",
      "howe 4.843476462893037\n",
      "howell 6.922918004572872\n",
      "however 5.218169912334447\n",
      "huge 6.006627272698717\n",
      "hugo 6.922918004572872\n",
      "human 5.418840607796598\n",
      "humanity 6.922918004572872\n",
      "humans 6.922918004572872\n",
      "hummh 6.922918004572872\n",
      "humor 6.229770824012927\n",
      "humorous 6.517452896464707\n",
      "humour 6.517452896464707\n",
      "hurt 6.922918004572872\n",
      "huston 6.229770824012927\n",
      "hype 6.922918004572872\n",
      "hypocrisy 6.922918004572872\n",
      "idea 5.824305715904762\n",
      "idealogical 6.922918004572872\n",
      "identified 6.922918004572872\n",
      "identifies 6.922918004572872\n",
      "identify 6.517452896464707\n",
      "idiot 6.229770824012927\n",
      "idiotic 6.922918004572872\n",
      "idyllic 6.922918004572872\n",
      "iffy 6.517452896464707\n",
      "im 3.0310977064622455\n",
      "imaginable 6.517452896464707\n",
      "imagination 6.006627272698717\n",
      "imaginative 6.517452896464707\n",
      "imagine 6.922918004572872\n",
      "imdb 6.922918004572872\n",
      "imitation 6.517452896464707\n",
      "impact 6.922918004572872\n",
      "imperial 6.922918004572872\n",
      "implausible 6.517452896464707\n",
      "important 6.517452896464707\n",
      "impossible 6.922918004572872\n",
      "impressed 6.517452896464707\n",
      "impression 6.922918004572872\n",
      "impressive 6.517452896464707\n",
      "improved 6.922918004572872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improvement 6.922918004572872\n",
      "improvisation 6.922918004572872\n",
      "impulse 6.922918004572872\n",
      "inappropriate 6.922918004572872\n",
      "incendiary 6.922918004572872\n",
      "includes 6.922918004572872\n",
      "including 6.229770824012927\n",
      "incomprehensible 6.922918004572872\n",
      "inconsistencies 6.922918004572872\n",
      "incorrectness 6.922918004572872\n",
      "incredible 6.006627272698717\n",
      "incredibly 6.229770824012927\n",
      "indeed 6.006627272698717\n",
      "indescribably 6.922918004572872\n",
      "indication 6.922918004572872\n",
      "indictment 6.922918004572872\n",
      "indie 6.922918004572872\n",
      "individual 6.922918004572872\n",
      "indoor 6.922918004572872\n",
      "indulgent 6.229770824012927\n",
      "industry 6.517452896464707\n",
      "ineptly 6.922918004572872\n",
      "inexperience 6.922918004572872\n",
      "inexplicable 6.922918004572872\n",
      "initially 6.922918004572872\n",
      "innocence 6.922918004572872\n",
      "insane 6.922918004572872\n",
      "inside 6.922918004572872\n",
      "insincere 6.922918004572872\n",
      "insipid 6.922918004572872\n",
      "insomniacs 6.922918004572872\n",
      "inspiration 6.922918004572872\n",
      "inspiring 6.922918004572872\n",
      "instant 6.922918004572872\n",
      "instead 6.517452896464707\n",
      "instruments 6.922918004572872\n",
      "insulin 6.922918004572872\n",
      "insult 6.006627272698717\n",
      "intangibles 6.922918004572872\n",
      "integral 6.922918004572872\n",
      "integration 6.922918004572872\n",
      "intelligence 6.006627272698717\n",
      "intelligent 6.006627272698717\n",
      "intense 6.922918004572872\n",
      "intensity 6.922918004572872\n",
      "intentions 6.922918004572872\n",
      "interacting 6.922918004572872\n",
      "interest 4.725693427236653\n",
      "interested 6.517452896464707\n",
      "interesting 4.9770078555175585\n",
      "interim 6.922918004572872\n",
      "interplay 6.517452896464707\n",
      "interpretations 6.922918004572872\n",
      "interview 6.922918004572872\n",
      "intoning 6.922918004572872\n",
      "intrigued 6.922918004572872\n",
      "inventive 6.922918004572872\n",
      "involved 5.824305715904762\n",
      "involves 6.922918004572872\n",
      "involving 6.922918004572872\n",
      "iq 6.229770824012927\n",
      "ireland 6.922918004572872\n",
      "ironically 6.922918004572872\n",
      "irons 6.517452896464707\n",
      "ironside 6.922918004572872\n",
      "irritating 6.922918004572872\n",
      "ishioka 6.922918004572872\n",
      "iso 5.670155036077504\n",
      "issue 6.229770824012927\n",
      "issues 6.517452896464707\n",
      "istagey 6.922918004572872\n",
      "italian 6.517452896464707\n",
      "ive 3.590713494397668\n",
      "jack 6.922918004572872\n",
      "jaclyn 6.922918004572872\n",
      "james 6.006627272698717\n",
      "jamie 6.517452896464707\n",
      "japanese 6.922918004572872\n",
      "jason 6.922918004572872\n",
      "jay 6.922918004572872\n",
      "jealousy 6.922918004572872\n",
      "jean 6.517452896464707\n",
      "jennifer 6.922918004572872\n",
      "jerky 6.517452896464707\n",
      "jerry 6.922918004572872\n",
      "jessica 6.922918004572872\n",
      "jessice 6.922918004572872\n",
      "jet 6.922918004572872\n",
      "jim 6.229770824012927\n",
      "jimmy 6.517452896464707\n",
      "job 5.131158535344817\n",
      "jobs 6.517452896464707\n",
      "joe 6.922918004572872\n",
      "john 5.824305715904762\n",
      "joins 6.922918004572872\n",
      "joke 6.229770824012927\n",
      "jokes 6.922918004572872\n",
      "jonah 6.517452896464707\n",
      "jones 6.922918004572872\n",
      "journey 6.517452896464707\n",
      "joy 4.525022731774501\n",
      "joyce 6.922918004572872\n",
      "juano 6.922918004572872\n",
      "judge 6.922918004572872\n",
      "judging 6.922918004572872\n",
      "judith 6.922918004572872\n",
      "judo 6.922918004572872\n",
      "julian 6.922918004572872\n",
      "june 6.517452896464707\n",
      "junk 6.517452896464707\n",
      "junkyard 6.922918004572872\n",
      "justice 6.922918004572872\n",
      "jutland 6.922918004572872\n",
      "kanaly 6.922918004572872\n",
      "kathy 6.922918004572872\n",
      "keep 5.670155036077504\n",
      "keeps 6.517452896464707\n",
      "keira 6.922918004572872\n",
      "keith 6.922918004572872\n",
      "kept 6.922918004572872\n",
      "kevin 6.922918004572872\n",
      "kid 5.218169912334447\n",
      "kidnapped 6.922918004572872\n",
      "kids 5.418840607796598\n",
      "kieslowski 6.922918004572872\n",
      "kill 5.536623643452981\n",
      "killer 6.517452896464707\n",
      "killing 6.229770824012927\n",
      "killings 6.922918004572872\n",
      "kind 5.218169912334447\n",
      "kinda 6.517452896464707\n",
      "kirk 6.922918004572872\n",
      "kitchy 6.922918004572872\n",
      "knew 6.229770824012927\n",
      "knightley 6.922918004572872\n",
      "knocked 6.922918004572872\n",
      "know 4.782851841076601\n",
      "known 6.517452896464707\n",
      "knows 6.922918004572872\n",
      "koteas 6.922918004572872\n",
      "kris 6.922918004572872\n",
      "kristoffersen 6.922918004572872\n",
      "kudos 6.922918004572872\n",
      "la 2.592184664286541\n",
      "labute 6.922918004572872\n",
      "lack 4.843476462893037\n",
      "lacked 6.517452896464707\n",
      "lacks 5.824305715904762\n",
      "ladies 6.922918004572872\n",
      "lady 6.922918004572872\n",
      "lame 5.824305715904762\n",
      "lance 5.418840607796598\n",
      "landscapes 6.922918004572872\n",
      "lane 6.229770824012927\n",
      "lange 6.517452896464707\n",
      "largely 6.922918004572872\n",
      "laselva 6.922918004572872\n",
      "lassie 6.922918004572872\n",
      "last 5.824305715904762\n",
      "lasting 6.922918004572872\n",
      "latched 6.922918004572872\n",
      "late 5.536623643452981\n",
      "later 6.922918004572872\n",
      "latest 6.922918004572872\n",
      "latifa 6.922918004572872\n",
      "latin 6.229770824012927\n",
      "latter 6.922918004572872\n",
      "laugh 5.536623643452981\n",
      "laughable 6.229770824012927\n",
      "laughed 6.922918004572872\n",
      "laughs 6.922918004572872\n",
      "layers 6.517452896464707\n",
      "lazy 6.517452896464707\n",
      "lead 5.670155036077504\n",
      "leading 6.922918004572872\n",
      "leap 6.922918004572872\n",
      "learn 6.229770824012927\n",
      "least 5.536623643452981\n",
      "leave 6.006627272698717\n",
      "leaves 6.517452896464707\n",
      "leaving 6.922918004572872\n",
      "lee 6.006627272698717\n",
      "left 6.229770824012927\n",
      "legal 6.922918004572872\n",
      "legendary 6.922918004572872\n",
      "length 6.517452896464707\n",
      "leni 6.922918004572872\n",
      "less 4.438011354784871\n",
      "lesser 6.517452896464707\n",
      "lestat 6.922918004572872\n",
      "let 4.782851841076601\n",
      "lets 6.517452896464707\n",
      "letting 6.922918004572872\n",
      "level 5.824305715904762\n",
      "levels 6.517452896464707\n",
      "lewis 6.922918004572872\n",
      "lid 5.536623643452981\n",
      "lie 4.214867803470662\n",
      "lies 6.229770824012927\n",
      "lieutenant 6.922918004572872\n",
      "life 4.9770078555175585\n",
      "lifetime 6.922918004572872\n",
      "light 5.05111582767128\n",
      "lighting 6.517452896464707\n",
      "like 3.7040421797046714\n",
      "liked 5.131158535344817\n",
      "likes 6.517452896464707\n",
      "lilli 6.922918004572872\n",
      "lilt 6.922918004572872\n",
      "limitations 6.922918004572872\n",
      "limited 6.922918004572872\n",
      "linda 6.922918004572872\n",
      "line 4.438011354784871\n",
      "linear 6.922918004572872\n",
      "lines 5.218169912334447\n",
      "lino 6.922918004572872\n",
      "lion 6.006627272698717\n",
      "list 5.536623643452981\n",
      "literally 6.517452896464707\n",
      "littered 6.922918004572872\n",
      "little 4.725693427236653\n",
      "lived 6.922918004572872\n",
      "lives 6.517452896464707\n",
      "living 6.229770824012927\n",
      "loads 6.517452896464707\n",
      "local 6.922918004572872\n",
      "location 6.229770824012927\n",
      "locations 6.922918004572872\n",
      "loewenhielm 6.922918004572872\n",
      "logic 6.229770824012927\n",
      "london 6.922918004572872\n",
      "loneliness 6.517452896464707\n",
      "long 4.9770078555175585\n",
      "longer 6.922918004572872\n",
      "look 4.283860674957613\n",
      "looked 6.006627272698717\n",
      "looking 6.229770824012927\n",
      "looks 6.229770824012927\n",
      "loose 6.517452896464707\n",
      "loosely 6.922918004572872\n",
      "lord 6.517452896464707\n",
      "los 4.9770078555175585\n",
      "losing 6.517452896464707\n",
      "lost 6.006627272698717\n",
      "lot 4.089704660516656\n",
      "lots 6.006627272698717\n",
      "lousy 6.229770824012927\n",
      "lovable 6.922918004572872\n",
      "love 4.182077980647671\n",
      "loved 5.3134800921387715\n",
      "lovely 6.006627272698717\n",
      "loves 6.922918004572872\n",
      "low 4.248769355146344\n",
      "lower 6.922918004572872\n",
      "loyalty 6.922918004572872\n",
      "lucio 6.922918004572872\n",
      "lucy 6.517452896464707\n",
      "lugosi 6.922918004572872\n",
      "lust 6.922918004572872\n",
      "luv 6.922918004572872\n",
      "lyrics 6.922918004572872\n",
      "macbeth 6.517452896464707\n",
      "machine 6.517452896464707\n",
      "mad 4.397189360264616\n",
      "made 4.480570969203668\n",
      "magnificent 6.922918004572872\n",
      "main 5.824305715904762\n",
      "mainly 6.922918004572872\n",
      "major 6.922918004572872\n",
      "make 4.150329282333091\n",
      "maker 6.229770824012927\n",
      "makers 6.922918004572872\n",
      "makes 5.418840607796598\n",
      "making 5.670155036077504\n",
      "male 5.824305715904762\n",
      "males 6.922918004572872\n",
      "malta 6.922918004572872\n",
      "man 3.6842395524084917\n",
      "managed 6.922918004572872\n",
      "manages 6.922918004572872\n",
      "manna 6.922918004572872\n",
      "mansonites 6.922918004572872\n",
      "many 4.908014984030608\n",
      "marbles 6.922918004572872\n",
      "march 6.922918004572872\n",
      "marine 6.922918004572872\n",
      "marion 6.922918004572872\n",
      "mark 5.824305715904762\n",
      "marred 6.922918004572872\n",
      "marriage 6.922918004572872\n",
      "martin 6.517452896464707\n",
      "masculine 6.922918004572872\n",
      "masculinity 6.922918004572872\n",
      "massive 6.922918004572872\n",
      "master 5.3134800921387715\n",
      "masterful 6.517452896464707\n",
      "masterpiece 6.229770824012927\n",
      "masterpieces 6.517452896464707\n",
      "material 6.517452896464707\n",
      "matrix 6.922918004572872\n",
      "matter 6.517452896464707\n",
      "matthews 6.922918004572872\n",
      "mature 6.229770824012927\n",
      "may 5.536623643452981\n",
      "maybe 6.006627272698717\n",
      "mchattie 6.922918004572872\n",
      "mclaglen 6.922918004572872\n",
      "meagre 6.922918004572872\n",
      "mean 5.218169912334447\n",
      "meanders 6.922918004572872\n",
      "meaning 6.006627272698717\n",
      "meanings 6.922918004572872\n",
      "meant 6.922918004572872\n",
      "medical 6.922918004572872\n",
      "mediocre 6.006627272698717\n",
      "meld 6.922918004572872\n",
      "melodrama 6.922918004572872\n",
      "melville 6.922918004572872\n",
      "member 5.670155036077504\n",
      "members 6.922918004572872\n",
      "memorable 5.670155036077504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memories 6.229770824012927\n",
      "memorized 6.922918004572872\n",
      "menace 6.922918004572872\n",
      "menacing 6.922918004572872\n",
      "mention 5.824305715904762\n",
      "mercy 6.517452896464707\n",
      "meredith 6.922918004572872\n",
      "merit 6.922918004572872\n",
      "mesmerising 6.922918004572872\n",
      "mess 5.418840607796598\n",
      "messages 6.922918004572872\n",
      "meteorite 6.922918004572872\n",
      "mexican 6.922918004572872\n",
      "michael 6.922918004572872\n",
      "mickey 6.006627272698717\n",
      "microsoft 6.922918004572872\n",
      "middle 6.922918004572872\n",
      "might 6.229770824012927\n",
      "mighty 6.922918004572872\n",
      "mind 5.536623643452981\n",
      "mindblowing 6.922918004572872\n",
      "miner 6.922918004572872\n",
      "mini 6.517452896464707\n",
      "minor 6.922918004572872\n",
      "minute 5.131158535344817\n",
      "minutes 5.418840607796598\n",
      "mirrormask 6.922918004572872\n",
      "miserable 6.922918004572872\n",
      "miserably 6.922918004572872\n",
      "mishima 6.517452896464707\n",
      "misplace 6.922918004572872\n",
      "miss 6.229770824012927\n",
      "missed 6.517452896464707\n",
      "mistakes 6.922918004572872\n",
      "miyazaki 6.229770824012927\n",
      "modern 6.517452896464707\n",
      "modest 6.922918004572872\n",
      "mollusk 6.922918004572872\n",
      "moment 5.670155036077504\n",
      "moments 6.517452896464707\n",
      "momentum 6.922918004572872\n",
      "money 5.824305715904762\n",
      "monica 6.922918004572872\n",
      "monolog 6.922918004572872\n",
      "monotonous 6.922918004572872\n",
      "monster 6.922918004572872\n",
      "monstrous 6.922918004572872\n",
      "monumental 6.922918004572872\n",
      "moral 6.517452896464707\n",
      "morgan 6.517452896464707\n",
      "morons 6.922918004572872\n",
      "mostly 5.670155036077504\n",
      "mother 6.517452896464707\n",
      "motion 5.824305715904762\n",
      "motivations 6.922918004572872\n",
      "mountain 6.922918004572872\n",
      "mouse 6.517452896464707\n",
      "mouth 6.517452896464707\n",
      "move 6.006627272698717\n",
      "moved 6.922918004572872\n",
      "movements 6.922918004572872\n",
      "moves 6.922918004572872\n",
      "movie 2.625632598354081\n",
      "movies 4.480570969203668\n",
      "moving 5.670155036077504\n",
      "ms 4.119557623666337\n",
      "much 4.725693427236653\n",
      "muddled 6.922918004572872\n",
      "muppets 6.922918004572872\n",
      "murder 6.006627272698717\n",
      "murdered 6.517452896464707\n",
      "murdering 6.922918004572872\n",
      "murky 6.922918004572872\n",
      "music 4.908014984030608\n",
      "musician 6.922918004572872\n",
      "must 5.418840607796598\n",
      "mystifying 6.922918004572872\n",
      "naked 6.922918004572872\n",
      "narration 6.922918004572872\n",
      "narrative 6.229770824012927\n",
      "nasty 6.922918004572872\n",
      "national 6.517452896464707\n",
      "nationalities 6.922918004572872\n",
      "native 6.229770824012927\n",
      "natural 6.229770824012927\n",
      "nature 6.517452896464707\n",
      "naughty 6.922918004572872\n",
      "nearly 6.922918004572872\n",
      "necklace 6.922918004572872\n",
      "need 5.670155036077504\n",
      "needed 6.229770824012927\n",
      "needlessly 6.922918004572872\n",
      "negative 6.229770824012927\n",
      "negulesco 6.922918004572872\n",
      "neighbour 6.922918004572872\n",
      "neil 6.517452896464707\n",
      "nerves 6.922918004572872\n",
      "nervous 6.922918004572872\n",
      "net 5.536623643452981\n",
      "netflix 6.922918004572872\n",
      "network 6.922918004572872\n",
      "never 4.671626205966376\n",
      "nevertheless 6.922918004572872\n",
      "nevsky 6.922918004572872\n",
      "new 5.418840607796598\n",
      "next 6.229770824012927\n",
      "nice 5.670155036077504\n",
      "nicola 6.922918004572872\n",
      "night 5.536623643452981\n",
      "nimoy 6.922918004572872\n",
      "nine 6.922918004572872\n",
      "no 2.390318511419616\n",
      "noble 6.922918004572872\n",
      "nobody 6.229770824012927\n",
      "noir 6.517452896464707\n",
      "non 5.218169912334447\n",
      "none 6.229770824012927\n",
      "nonetheless 6.922918004572872\n",
      "nonsense 6.229770824012927\n",
      "nor 5.670155036077504\n",
      "normally 6.922918004572872\n",
      "northern 6.922918004572872\n",
      "nostalgia 6.922918004572872\n",
      "not 2.7408678619316658\n",
      "notable 6.517452896464707\n",
      "notch 6.922918004572872\n",
      "note 5.824305715904762\n",
      "noteworthy 6.517452896464707\n",
      "nothing 5.131158535344817\n",
      "novella 6.922918004572872\n",
      "number 6.517452896464707\n",
      "numbers 6.922918004572872\n",
      "nun 6.922918004572872\n",
      "nuns 6.922918004572872\n",
      "nurse 6.922918004572872\n",
      "nut 4.908014984030608\n",
      "nuts 6.517452896464707\n",
      "obliged 6.922918004572872\n",
      "obsessed 6.922918004572872\n",
      "obvious 6.229770824012927\n",
      "obviously 6.229770824012927\n",
      "occasionally 6.006627272698717\n",
      "occupied 6.517452896464707\n",
      "occur 6.517452896464707\n",
      "occurs 6.922918004572872\n",
      "odd 6.229770824012927\n",
      "offend 6.922918004572872\n",
      "offensive 6.517452896464707\n",
      "offer 6.006627272698717\n",
      "offers 6.229770824012927\n",
      "often 6.006627272698717\n",
      "oh 5.05111582767128\n",
      "okay 6.922918004572872\n",
      "old 4.671626205966376\n",
      "olde 6.517452896464707\n",
      "older 6.922918004572872\n",
      "ole 4.320228319128488\n",
      "olivia 6.922918004572872\n",
      "omit 6.006627272698717\n",
      "one 3.094276608083777\n",
      "ones 5.824305715904762\n",
      "open 5.824305715904762\n",
      "opened 6.517452896464707\n",
      "opening 6.517452896464707\n",
      "operas 6.922918004572872\n",
      "opinion 6.922918004572872\n",
      "ordeal 6.922918004572872\n",
      "oriented 6.922918004572872\n",
      "original 5.670155036077504\n",
      "originality 6.922918004572872\n",
      "origins 6.922918004572872\n",
      "ortolani 6.922918004572872\n",
      "oscar 6.229770824012927\n",
      "others 6.517452896464707\n",
      "otherwise 6.922918004572872\n",
      "ought 4.671626205966376\n",
      "outlandish 6.922918004572872\n",
      "outlets 6.922918004572872\n",
      "outside 6.922918004572872\n",
      "outward 6.922918004572872\n",
      "overacting 6.517452896464707\n",
      "overall 5.824305715904762\n",
      "overcome 6.922918004572872\n",
      "overdue 6.922918004572872\n",
      "overly 6.517452896464707\n",
      "overs 6.517452896464707\n",
      "overt 6.922918004572872\n",
      "overwrought 6.922918004572872\n",
      "owed 5.670155036077504\n",
      "owls 6.922918004572872\n",
      "owned 6.517452896464707\n",
      "owns 6.229770824012927\n",
      "oy 4.089704660516656\n",
      "pace 5.3134800921387715\n",
      "paced 6.229770824012927\n",
      "pacing 6.517452896464707\n",
      "pack 6.922918004572872\n",
      "paid 6.517452896464707\n",
      "painful 6.229770824012927\n",
      "painfully 6.922918004572872\n",
      "paint 6.517452896464707\n",
      "painted 6.922918004572872\n",
      "pair 6.517452896464707\n",
      "palance 6.922918004572872\n",
      "pandering 6.922918004572872\n",
      "pans 6.517452896464707\n",
      "paolo 6.922918004572872\n",
      "pap 6.517452896464707\n",
      "paper 6.922918004572872\n",
      "par 4.248769355146344\n",
      "parents 6.517452896464707\n",
      "park 6.517452896464707\n",
      "parker 6.922918004572872\n",
      "part 4.397189360264616\n",
      "partaking 6.922918004572872\n",
      "particular 5.418840607796598\n",
      "particularly 5.824305715904762\n",
      "parts 6.006627272698717\n",
      "passed 6.922918004572872\n",
      "passion 6.922918004572872\n",
      "past 6.517452896464707\n",
      "patent 6.922918004572872\n",
      "pathetic 5.824305715904762\n",
      "patriotism 6.922918004572872\n",
      "paul 6.229770824012927\n",
      "pay 6.922918004572872\n",
      "peaking 6.517452896464707\n",
      "pearls 6.922918004572872\n",
      "peculiarity 6.922918004572872\n",
      "pedestal 6.922918004572872\n",
      "pencil 6.922918004572872\n",
      "people 4.9770078555175585\n",
      "perabo 6.922918004572872\n",
      "perfect 5.218169912334447\n",
      "perfected 6.922918004572872\n",
      "perfectly 6.229770824012927\n",
      "performance 4.843476462893037\n",
      "performances 6.006627272698717\n",
      "perhaps 6.006627272698717\n",
      "period 6.006627272698717\n",
      "perplexing 6.922918004572872\n",
      "person 6.229770824012927\n",
      "personalities 6.922918004572872\n",
      "personally 6.922918004572872\n",
      "peter 6.229770824012927\n",
      "pg 6.517452896464707\n",
      "phantasm 6.922918004572872\n",
      "phenomenal 6.922918004572872\n",
      "philippa 6.922918004572872\n",
      "phony 6.922918004572872\n",
      "photograph 6.006627272698717\n",
      "photography 6.229770824012927\n",
      "phrase 6.922918004572872\n",
      "physical 6.922918004572872\n",
      "pi 3.590713494397668\n",
      "picked 6.922918004572872\n",
      "picture 5.824305715904762\n",
      "pictures 6.517452896464707\n",
      "piece 4.908014984030608\n",
      "pieces 6.006627272698717\n",
      "pile 6.922918004572872\n",
      "pillow 6.922918004572872\n",
      "pitch 6.922918004572872\n",
      "pitiful 6.517452896464707\n",
      "pixar 6.922918004572872\n",
      "place 5.418840607796598\n",
      "places 6.229770824012927\n",
      "plain 6.006627272698717\n",
      "plane 6.922918004572872\n",
      "planned 6.922918004572872\n",
      "plants 6.922918004572872\n",
      "play 4.119557623666337\n",
      "played 5.218169912334447\n",
      "player 6.229770824012927\n",
      "players 6.922918004572872\n",
      "playing 5.536623643452981\n",
      "plays 6.229770824012927\n",
      "pleasant 6.517452896464707\n",
      "pleased 6.517452896464707\n",
      "pleaser 6.517452896464707\n",
      "pleasing 6.922918004572872\n",
      "pledge 6.922918004572872\n",
      "plenty 6.517452896464707\n",
      "plmer 6.922918004572872\n",
      "plot 4.320228319128488\n",
      "plug 6.922918004572872\n",
      "plus 6.229770824012927\n",
      "pm 6.229770824012927\n",
      "poet 6.229770824012927\n",
      "poetry 6.517452896464707\n",
      "poignant 6.922918004572872\n",
      "point 4.782851841076601\n",
      "pointillistic 6.922918004572872\n",
      "pointless 6.229770824012927\n",
      "poised 6.922918004572872\n",
      "poler 6.922918004572872\n",
      "political 6.517452896464707\n",
      "politically 6.922918004572872\n",
      "politics 6.922918004572872\n",
      "ponyo 6.922918004572872\n",
      "poor 5.536623643452981\n",
      "poorly 6.517452896464707\n",
      "popcorn 6.922918004572872\n",
      "popular 6.517452896464707\n",
      "portrayal 5.536623643452981\n",
      "portrayals 6.922918004572872\n",
      "portrayed 6.922918004572872\n",
      "portraying 6.517452896464707\n",
      "positive 6.922918004572872\n",
      "possible 6.006627272698717\n",
      "possibly 6.229770824012927\n",
      "post 6.922918004572872\n",
      "potted 6.922918004572872\n",
      "power 6.006627272698717\n",
      "powerful 6.922918004572872\n",
      "powerhouse 6.922918004572872\n",
      "practical 6.517452896464707\n",
      "practically 6.922918004572872\n",
      "pray 6.922918004572872\n",
      "precisely 6.922918004572872\n",
      "predict 5.05111582767128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictable 5.218169912334447\n",
      "predictably 6.517452896464707\n",
      "prejudice 6.922918004572872\n",
      "prelude 6.922918004572872\n",
      "premise 6.006627272698717\n",
      "prepared 6.922918004572872\n",
      "presence 6.517452896464707\n",
      "presents 6.006627272698717\n",
      "preservation 6.922918004572872\n",
      "president 6.922918004572872\n",
      "pretentious 6.229770824012927\n",
      "pretext 6.922918004572872\n",
      "pretty 4.9770078555175585\n",
      "previous 6.517452896464707\n",
      "primal 6.922918004572872\n",
      "primary 6.922918004572872\n",
      "probably 5.536623643452981\n",
      "problem 5.670155036077504\n",
      "problems 6.006627272698717\n",
      "proceedings 6.517452896464707\n",
      "process 6.517452896464707\n",
      "produce 5.824305715904762\n",
      "produced 6.229770824012927\n",
      "producer 6.517452896464707\n",
      "producers 6.922918004572872\n",
      "product 5.670155036077504\n",
      "production 5.824305715904762\n",
      "professionals 6.922918004572872\n",
      "professor 6.922918004572872\n",
      "progresses 6.922918004572872\n",
      "promote 6.922918004572872\n",
      "prompted 6.922918004572872\n",
      "prone 6.922918004572872\n",
      "propaganda 6.922918004572872\n",
      "properly 6.922918004572872\n",
      "proud 6.229770824012927\n",
      "proudly 6.922918004572872\n",
      "provided 6.922918004572872\n",
      "provokes 6.922918004572872\n",
      "provoking 6.229770824012927\n",
      "ps 4.480570969203668\n",
      "pseudo 6.922918004572872\n",
      "psychological 6.517452896464707\n",
      "psychotic 6.922918004572872\n",
      "public 6.922918004572872\n",
      "pull 6.006627272698717\n",
      "pulling 6.922918004572872\n",
      "pulls 6.922918004572872\n",
      "punched 6.922918004572872\n",
      "punches 6.922918004572872\n",
      "punish 6.517452896464707\n",
      "punishment 6.922918004572872\n",
      "puppet 6.517452896464707\n",
      "puppets 6.517452896464707\n",
      "pure 6.229770824012927\n",
      "purity 6.922918004572872\n",
      "put 5.3134800921387715\n",
      "putting 6.517452896464707\n",
      "puzzle 6.922918004572872\n",
      "pyromaniac 6.922918004572872\n",
      "qu 4.150329282333091\n",
      "quaid 6.922918004572872\n",
      "qualities 6.922918004572872\n",
      "quality 6.229770824012927\n",
      "question 6.229770824012927\n",
      "questioning 6.922918004572872\n",
      "quick 6.517452896464707\n",
      "quicker 6.922918004572872\n",
      "quiet 6.922918004572872\n",
      "quinn 6.517452896464707\n",
      "quite 5.3134800921387715\n",
      "race 5.536623643452981\n",
      "racial 6.517452896464707\n",
      "racism 6.517452896464707\n",
      "radiant 6.922918004572872\n",
      "raging 6.922918004572872\n",
      "random 6.922918004572872\n",
      "range 6.229770824012927\n",
      "ranks 6.229770824012927\n",
      "rare 6.922918004572872\n",
      "rate 5.05111582767128\n",
      "rated 6.229770824012927\n",
      "rather 5.670155036077504\n",
      "rating 5.536623643452981\n",
      "ratings 6.517452896464707\n",
      "raver 6.922918004572872\n",
      "raw 5.824305715904762\n",
      "ray 4.843476462893037\n",
      "reactions 6.517452896464707\n",
      "readers 6.922918004572872\n",
      "reading 6.517452896464707\n",
      "ready 5.536623643452981\n",
      "real 3.573013917298267\n",
      "realised 6.922918004572872\n",
      "realistic 6.229770824012927\n",
      "reality 6.517452896464707\n",
      "realize 6.517452896464707\n",
      "realized 6.922918004572872\n",
      "really 4.089704660516656\n",
      "reason 5.824305715904762\n",
      "reasonable 6.922918004572872\n",
      "reasons 6.922918004572872\n",
      "receive 6.229770824012927\n",
      "received 6.517452896464707\n",
      "recent 5.824305715904762\n",
      "recently 6.517452896464707\n",
      "recommend 4.671626205966376\n",
      "recommended 5.536623643452981\n",
      "reconciliation 6.922918004572872\n",
      "recover 6.922918004572872\n",
      "recurring 6.922918004572872\n",
      "redeemed 6.922918004572872\n",
      "redeeming 6.517452896464707\n",
      "reenactments 6.922918004572872\n",
      "references 6.517452896464707\n",
      "reflected 6.922918004572872\n",
      "refreshing 6.922918004572872\n",
      "regardless 6.922918004572872\n",
      "regret 6.006627272698717\n",
      "regrettable 6.922918004572872\n",
      "regrettably 6.922918004572872\n",
      "rejection 6.922918004572872\n",
      "relate 6.517452896464707\n",
      "related 6.922918004572872\n",
      "relation 5.670155036077504\n",
      "relations 5.824305715904762\n",
      "relationship 6.229770824012927\n",
      "relationships 6.517452896464707\n",
      "relatively 6.922918004572872\n",
      "relaxing 6.922918004572872\n",
      "release 6.006627272698717\n",
      "released 6.229770824012927\n",
      "relief 6.922918004572872\n",
      "relying 6.922918004572872\n",
      "remaining 6.517452896464707\n",
      "remake 6.229770824012927\n",
      "remarkable 6.517452896464707\n",
      "remember 6.229770824012927\n",
      "reminded 6.922918004572872\n",
      "remotely 6.517452896464707\n",
      "removing 6.922918004572872\n",
      "rendering 6.922918004572872\n",
      "rendition 6.922918004572872\n",
      "renowned 6.922918004572872\n",
      "rent 4.843476462893037\n",
      "repair 6.922918004572872\n",
      "repeated 6.922918004572872\n",
      "repeating 6.922918004572872\n",
      "repeats 6.922918004572872\n",
      "repertory 6.922918004572872\n",
      "reporter 6.922918004572872\n",
      "represents 6.922918004572872\n",
      "require 6.922918004572872\n",
      "rescue 6.922918004572872\n",
      "researched 6.922918004572872\n",
      "resounding 6.922918004572872\n",
      "respecting 6.922918004572872\n",
      "rest 4.525022731774501\n",
      "restrained 6.517452896464707\n",
      "result 6.229770824012927\n",
      "results 6.922918004572872\n",
      "resume 6.922918004572872\n",
      "retarded 6.922918004572872\n",
      "retreat 6.922918004572872\n",
      "return 6.517452896464707\n",
      "revealing 6.517452896464707\n",
      "revenge 6.922918004572872\n",
      "revere 6.922918004572872\n",
      "reverse 6.922918004572872\n",
      "review 5.670155036077504\n",
      "reviewer 6.229770824012927\n",
      "reviewers 6.922918004572872\n",
      "reviews 6.922918004572872\n",
      "rice 6.922918004572872\n",
      "rickman 6.922918004572872\n",
      "ridiculous 5.824305715904762\n",
      "ridiculousness 6.922918004572872\n",
      "right 4.782851841076601\n",
      "riot 6.517452896464707\n",
      "rips 6.922918004572872\n",
      "rise 6.006627272698717\n",
      "rita 6.517452896464707\n",
      "rivalry 6.922918004572872\n",
      "riveted 6.922918004572872\n",
      "riz 6.517452896464707\n",
      "road 6.229770824012927\n",
      "robert 6.517452896464707\n",
      "robotic 6.922918004572872\n",
      "rochon 6.922918004572872\n",
      "rocked 6.922918004572872\n",
      "rocks 6.922918004572872\n",
      "roeg 6.922918004572872\n",
      "role 5.418840607796598\n",
      "roles 6.006627272698717\n",
      "roller 6.922918004572872\n",
      "rolls 6.922918004572872\n",
      "romantic 6.922918004572872\n",
      "room 6.006627272698717\n",
      "roosevelt 6.922918004572872\n",
      "roth 6.517452896464707\n",
      "rough 5.131158535344817\n",
      "round 5.218169912334447\n",
      "routine 6.922918004572872\n",
      "row 5.3134800921387715\n",
      "rpg 6.922918004572872\n",
      "rpger 6.922918004572872\n",
      "rubbish 6.922918004572872\n",
      "rubin 6.922918004572872\n",
      "rumbles 6.922918004572872\n",
      "run 6.006627272698717\n",
      "running 6.517452896464707\n",
      "ruthless 6.517452896464707\n",
      "ryan 6.517452896464707\n",
      "ryans 6.922918004572872\n",
      "sabotages 6.922918004572872\n",
      "sack 6.922918004572872\n",
      "sacrifice 6.922918004572872\n",
      "sad 6.922918004572872\n",
      "said 6.006627272698717\n",
      "sake 6.517452896464707\n",
      "salesman 6.922918004572872\n",
      "sam 6.517452896464707\n",
      "sample 6.922918004572872\n",
      "sand 5.824305715904762\n",
      "sandra 6.517452896464707\n",
      "sappiest 6.922918004572872\n",
      "sarcophage 6.922918004572872\n",
      "sat 5.824305715904762\n",
      "satanic 6.922918004572872\n",
      "savalas 6.922918004572872\n",
      "savant 6.922918004572872\n",
      "save 6.922918004572872\n",
      "savor 6.922918004572872\n",
      "saw 5.05111582767128\n",
      "say 5.05111582767128\n",
      "says 6.922918004572872\n",
      "scale 6.229770824012927\n",
      "scamp 6.922918004572872\n",
      "scare 5.824305715904762\n",
      "scared 6.229770824012927\n",
      "scares 6.922918004572872\n",
      "scary 6.517452896464707\n",
      "scene 4.283860674957613\n",
      "scenery 6.229770824012927\n",
      "scenes 4.782851841076601\n",
      "schilling 6.922918004572872\n",
      "schizophrenic 6.922918004572872\n",
      "school 6.517452896464707\n",
      "schoolers 6.922918004572872\n",
      "schrader 6.517452896464707\n",
      "schultz 6.922918004572872\n",
      "sci 5.418840607796598\n",
      "science 6.922918004572872\n",
      "scientist 6.922918004572872\n",
      "score 6.229770824012927\n",
      "scot 6.922918004572872\n",
      "scream 6.517452896464707\n",
      "screamy 6.922918004572872\n",
      "screen 4.908014984030608\n",
      "screened 6.922918004572872\n",
      "screenplay 6.922918004572872\n",
      "screenwriter 6.229770824012927\n",
      "scrimm 6.922918004572872\n",
      "script 4.620332911578826\n",
      "scripted 6.922918004572872\n",
      "scripting 6.922918004572872\n",
      "scripts 6.922918004572872\n",
      "sculpture 6.922918004572872\n",
      "sea 5.3134800921387715\n",
      "seamless 6.517452896464707\n",
      "seamlessly 6.922918004572872\n",
      "sean 6.922918004572872\n",
      "season 6.517452896464707\n",
      "seat 6.922918004572872\n",
      "second 5.418840607796598\n",
      "secondary 6.922918004572872\n",
      "secondly 6.922918004572872\n",
      "see 3.3965574799567104\n",
      "seeing 5.536623643452981\n",
      "seem 5.218169912334447\n",
      "seemed 6.229770824012927\n",
      "seems 6.229770824012927\n",
      "seen 4.480570969203668\n",
      "selections 6.922918004572872\n",
      "self 5.670155036077504\n",
      "sells 6.922918004572872\n",
      "semi 6.922918004572872\n",
      "senior 6.922918004572872\n",
      "sense 5.536623643452981\n",
      "senses 6.517452896464707\n",
      "sensibility 6.922918004572872\n",
      "sensitivities 6.922918004572872\n",
      "sentiment 6.517452896464707\n",
      "seperate 6.922918004572872\n",
      "sequel 6.229770824012927\n",
      "sequels 6.922918004572872\n",
      "sequence 6.006627272698717\n",
      "sequences 6.517452896464707\n",
      "series 5.418840607796598\n",
      "serious 5.536623643452981\n",
      "seriously 6.229770824012927\n",
      "served 6.229770824012927\n",
      "set 5.218169912334447\n",
      "sets 6.006627272698717\n",
      "setting 6.229770824012927\n",
      "settings 6.922918004572872\n",
      "seuss 6.922918004572872\n",
      "several 6.517452896464707\n",
      "sex 6.922918004572872\n",
      "shakespear 6.517452896464707\n",
      "shakespears 6.922918004572872\n",
      "shallow 6.922918004572872\n",
      "shame 6.229770824012927\n",
      "shameful 6.922918004572872\n",
      "share 6.229770824012927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sharing 6.922918004572872\n",
      "sharply 6.922918004572872\n",
      "shatner 6.922918004572872\n",
      "shattered 6.922918004572872\n",
      "shed 5.536623643452981\n",
      "sheer 6.922918004572872\n",
      "shelf 6.922918004572872\n",
      "shell 6.922918004572872\n",
      "shelves 6.922918004572872\n",
      "shenanigans 6.922918004572872\n",
      "shepard 6.922918004572872\n",
      "shined 6.922918004572872\n",
      "shirley 6.922918004572872\n",
      "shocking 6.922918004572872\n",
      "shooting 6.922918004572872\n",
      "short 5.05111582767128\n",
      "shortlist 6.922918004572872\n",
      "shot 5.418840607796598\n",
      "shots 6.229770824012927\n",
      "show 4.397189360264616\n",
      "showcasing 6.922918004572872\n",
      "showed 6.229770824012927\n",
      "shows 6.229770824012927\n",
      "shut 6.922918004572872\n",
      "sibling 6.922918004572872\n",
      "sick 6.517452896464707\n",
      "side 4.725693427236653\n",
      "sidelined 6.922918004572872\n",
      "sign 5.670155036077504\n",
      "significant 6.517452896464707\n",
      "silent 6.229770824012927\n",
      "silly 6.922918004572872\n",
      "simmering 6.922918004572872\n",
      "simplifying 6.922918004572872\n",
      "simply 5.418840607796598\n",
      "since 5.536623643452981\n",
      "sincere 6.517452896464707\n",
      "sing 4.357968647111335\n",
      "singing 6.229770824012927\n",
      "single 5.824305715904762\n",
      "sinister 6.922918004572872\n",
      "sink 6.517452896464707\n",
      "sinking 6.922918004572872\n",
      "sister 6.229770824012927\n",
      "sisters 6.517452896464707\n",
      "sit 4.908014984030608\n",
      "sitcoms 6.922918004572872\n",
      "site 6.006627272698717\n",
      "sites 6.922918004572872\n",
      "sits 6.922918004572872\n",
      "situation 6.229770824012927\n",
      "situations 6.922918004572872\n",
      "skilled 6.922918004572872\n",
      "skip 6.922918004572872\n",
      "slackers 6.922918004572872\n",
      "slavic 6.922918004572872\n",
      "sleep 6.229770824012927\n",
      "slideshow 6.922918004572872\n",
      "slightest 6.517452896464707\n",
      "slightly 6.922918004572872\n",
      "slimy 6.922918004572872\n",
      "sloppy 6.922918004572872\n",
      "slow 5.536623643452981\n",
      "slurs 6.922918004572872\n",
      "smack 6.922918004572872\n",
      "small 6.006627272698717\n",
      "smart 6.229770824012927\n",
      "smells 6.922918004572872\n",
      "smile 6.517452896464707\n",
      "smiling 6.922918004572872\n",
      "smith 6.922918004572872\n",
      "smoothly 6.922918004572872\n",
      "snider 6.922918004572872\n",
      "snow 6.922918004572872\n",
      "soap 6.922918004572872\n",
      "sobering 6.922918004572872\n",
      "social 6.922918004572872\n",
      "soldiers 6.922918004572872\n",
      "sole 6.922918004572872\n",
      "solid 5.824305715904762\n",
      "solidifying 6.922918004572872\n",
      "solving 6.922918004572872\n",
      "someone 6.229770824012927\n",
      "something 6.006627272698717\n",
      "sometimes 6.006627272698717\n",
      "somewhat 6.517452896464707\n",
      "son 4.357968647111335\n",
      "song 5.536623643452981\n",
      "songs 6.006627272698717\n",
      "soon 6.922918004572872\n",
      "sophisticated 6.922918004572872\n",
      "sorrentino 6.922918004572872\n",
      "sorry 6.922918004572872\n",
      "sort 6.229770824012927\n",
      "soul 6.517452896464707\n",
      "sound 4.9770078555175585\n",
      "sounded 6.922918004572872\n",
      "sounds 6.922918004572872\n",
      "soundtrack 6.006627272698717\n",
      "sour 6.922918004572872\n",
      "south 6.006627272698717\n",
      "southern 6.922918004572872\n",
      "space 5.824305715904762\n",
      "spacek 6.922918004572872\n",
      "spacey 6.922918004572872\n",
      "span 6.922918004572872\n",
      "speak 6.006627272698717\n",
      "speaking 6.922918004572872\n",
      "special 4.782851841076601\n",
      "speed 6.517452896464707\n",
      "spend 6.922918004572872\n",
      "spent 6.229770824012927\n",
      "spew 6.922918004572872\n",
      "sphere 6.517452896464707\n",
      "spiffy 6.922918004572872\n",
      "splendid 6.922918004572872\n",
      "spock 6.922918004572872\n",
      "spoil 5.670155036077504\n",
      "spoiled 6.922918004572872\n",
      "spoiler 6.006627272698717\n",
      "spoilers 6.229770824012927\n",
      "spot 6.922918004572872\n",
      "spy 6.922918004572872\n",
      "squibs 6.922918004572872\n",
      "stable 6.922918004572872\n",
      "stage 6.517452896464707\n",
      "stagy 6.922918004572872\n",
      "stand 4.9770078555175585\n",
      "standout 6.922918004572872\n",
      "stanwyck 6.229770824012927\n",
      "star 4.843476462893037\n",
      "starlet 6.922918004572872\n",
      "starring 6.922918004572872\n",
      "stars 6.229770824012927\n",
      "start 5.536623643452981\n",
      "started 5.824305715904762\n",
      "starts 6.922918004572872\n",
      "state 5.824305715904762\n",
      "stay 6.229770824012927\n",
      "stayed 6.922918004572872\n",
      "stealing 6.922918004572872\n",
      "steamboat 6.922918004572872\n",
      "steele 6.922918004572872\n",
      "step 6.229770824012927\n",
      "stephen 6.922918004572872\n",
      "stereotypes 6.517452896464707\n",
      "stereotypically 6.922918004572872\n",
      "steve 6.229770824012927\n",
      "stewart 6.922918004572872\n",
      "stick 6.922918004572872\n",
      "still 4.908014984030608\n",
      "stinker 6.922918004572872\n",
      "stinks 6.922918004572872\n",
      "stocking 6.517452896464707\n",
      "stockings 6.922918004572872\n",
      "stoic 6.922918004572872\n",
      "store 6.517452896464707\n",
      "stories 6.006627272698717\n",
      "storm 6.922918004572872\n",
      "story 4.283860674957613\n",
      "storyline 6.006627272698717\n",
      "storytelling 6.922918004572872\n",
      "stowe 6.922918004572872\n",
      "strange 6.517452896464707\n",
      "stranger 6.922918004572872\n",
      "stratus 6.922918004572872\n",
      "straw 6.922918004572872\n",
      "street 6.517452896464707\n",
      "strident 6.922918004572872\n",
      "string 6.922918004572872\n",
      "strives 6.922918004572872\n",
      "strokes 6.922918004572872\n",
      "strong 5.824305715904762\n",
      "struck 6.922918004572872\n",
      "structure 6.922918004572872\n",
      "struggle 6.922918004572872\n",
      "stuart 6.922918004572872\n",
      "student 6.517452896464707\n",
      "students 6.922918004572872\n",
      "studio 6.517452896464707\n",
      "study 6.922918004572872\n",
      "stuff 6.922918004572872\n",
      "stunning 6.517452896464707\n",
      "stupid 4.908014984030608\n",
      "stupidity 6.922918004572872\n",
      "style 5.670155036077504\n",
      "stylized 6.922918004572872\n",
      "sub 4.9770078555175585\n",
      "subject 6.922918004572872\n",
      "subjects 6.922918004572872\n",
      "sublime 6.229770824012927\n",
      "sublimely 6.922918004572872\n",
      "subplots 6.922918004572872\n",
      "subtitles 6.922918004572872\n",
      "subtle 5.670155036077504\n",
      "subversive 6.922918004572872\n",
      "subverting 6.922918004572872\n",
      "succeeded 6.922918004572872\n",
      "succeeds 6.922918004572872\n",
      "success 6.922918004572872\n",
      "suck 5.824305715904762\n",
      "sucked 6.229770824012927\n",
      "sucks 6.922918004572872\n",
      "suffered 6.922918004572872\n",
      "suffering 6.517452896464707\n",
      "suggest 6.517452896464707\n",
      "suggests 6.922918004572872\n",
      "suited 6.922918004572872\n",
      "sum 6.006627272698717\n",
      "summary 6.517452896464707\n",
      "sundays 6.922918004572872\n",
      "super 4.9770078555175585\n",
      "superb 5.418840607796598\n",
      "superbad 6.922918004572872\n",
      "superbly 6.517452896464707\n",
      "superficial 6.922918004572872\n",
      "superlative 6.922918004572872\n",
      "supernatural 6.922918004572872\n",
      "supporting 6.922918004572872\n",
      "supposed 6.006627272698717\n",
      "supposedly 6.517452896464707\n",
      "sure 5.824305715904762\n",
      "surely 6.922918004572872\n",
      "surf 6.517452896464707\n",
      "surface 6.922918004572872\n",
      "surprised 6.922918004572872\n",
      "surprises 6.922918004572872\n",
      "surprising 5.824305715904762\n",
      "surprisingly 6.229770824012927\n",
      "surrounding 6.229770824012927\n",
      "surroundings 6.922918004572872\n",
      "survivors 6.922918004572872\n",
      "suspense 5.418840607796598\n",
      "suspension 6.922918004572872\n",
      "sven 6.922918004572872\n",
      "swamp 6.922918004572872\n",
      "sweep 6.922918004572872\n",
      "sweet 6.922918004572872\n",
      "switched 6.922918004572872\n",
      "swords 6.922918004572872\n",
      "sydney 6.922918004572872\n",
      "sympathetic 6.922918004572872\n",
      "syrupy 6.922918004572872\n",
      "system 6.922918004572872\n",
      "tacky 6.922918004572872\n",
      "taelons 6.922918004572872\n",
      "take 5.218169912334447\n",
      "taken 6.922918004572872\n",
      "takes 6.229770824012927\n",
      "taking 6.229770824012927\n",
      "tale 5.3134800921387715\n",
      "talent 5.824305715904762\n",
      "talented 6.229770824012927\n",
      "talents 6.922918004572872\n",
      "talk 5.824305715904762\n",
      "tanks 6.922918004572872\n",
      "taped 6.517452896464707\n",
      "tardis 6.922918004572872\n",
      "task 6.922918004572872\n",
      "taste 6.922918004572872\n",
      "taxidermists 6.922918004572872\n",
      "taylor 6.517452896464707\n",
      "teacher 6.922918004572872\n",
      "teaches 6.922918004572872\n",
      "team 6.517452896464707\n",
      "tear 6.517452896464707\n",
      "tears 6.922918004572872\n",
      "technically 6.922918004572872\n",
      "teddy 6.922918004572872\n",
      "tedium 6.922918004572872\n",
      "teen 6.229770824012927\n",
      "teenagers 6.922918004572872\n",
      "teeth 6.922918004572872\n",
      "telephone 6.922918004572872\n",
      "television 6.922918004572872\n",
      "tell 5.05111582767128\n",
      "telly 6.922918004572872\n",
      "temperaments 6.922918004572872\n",
      "ten 4.248769355146344\n",
      "tender 6.922918004572872\n",
      "tension 6.229770824012927\n",
      "tensions 6.922918004572872\n",
      "terminology 6.922918004572872\n",
      "terms 6.517452896464707\n",
      "terrible 5.218169912334447\n",
      "terribly 6.517452896464707\n",
      "terrific 6.006627272698717\n",
      "terror 6.922918004572872\n",
      "th 2.4570098859182883\n",
      "thanks 6.922918004572872\n",
      "theater 6.006627272698717\n",
      "theatre 6.517452896464707\n",
      "theatres 6.922918004572872\n",
      "theatrical 6.922918004572872\n",
      "theme 6.006627272698717\n",
      "themes 6.517452896464707\n",
      "therapy 6.922918004572872\n",
      "thick 6.922918004572872\n",
      "thing 3.7242448870221905\n",
      "things 5.05111582767128\n",
      "think 4.571542747409394\n",
      "thinking 6.229770824012927\n",
      "thomerson 6.922918004572872\n",
      "thoroughly 6.229770824012927\n",
      "thorsen 6.922918004572872\n",
      "though 4.283860674957613\n",
      "thought 4.908014984030608\n",
      "thoughts 6.517452896464707\n",
      "thousand 6.922918004572872\n",
      "thread 6.922918004572872\n",
      "three 6.922918004572872\n",
      "threshold 6.922918004572872\n",
      "thrilled 6.517452896464707\n",
      "thriller 6.006627272698717\n",
      "thrillers 6.922918004572872\n",
      "throughout 5.670155036077504\n",
      "throwback 6.922918004572872\n",
      "thrown 6.229770824012927\n",
      "thug 6.922918004572872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thumper 6.922918004572872\n",
      "thunderbirds 6.922918004572872\n",
      "thus 6.922918004572872\n",
      "ticker 6.922918004572872\n",
      "tickets 6.922918004572872\n",
      "tightly 6.922918004572872\n",
      "time 3.6457732715806954\n",
      "timeless 6.922918004572872\n",
      "timely 6.922918004572872\n",
      "timers 6.922918004572872\n",
      "times 5.418840607796598\n",
      "timing 6.922918004572872\n",
      "tiny 6.922918004572872\n",
      "tired 6.922918004572872\n",
      "title 6.517452896464707\n",
      "titta 6.922918004572872\n",
      "today 6.006627272698717\n",
      "together 5.536623643452981\n",
      "told 6.229770824012927\n",
      "tolerable 6.922918004572872\n",
      "tolerate 6.922918004572872\n",
      "tom 5.536623643452981\n",
      "tomorrow 6.922918004572872\n",
      "tone 6.517452896464707\n",
      "tongue 6.922918004572872\n",
      "tonight 6.922918004572872\n",
      "tons 6.922918004572872\n",
      "tony 6.922918004572872\n",
      "took 6.517452896464707\n",
      "toons 6.517452896464707\n",
      "top 5.3134800921387715\n",
      "tops 6.922918004572872\n",
      "torture 5.824305715904762\n",
      "tortured 6.922918004572872\n",
      "total 5.05111582767128\n",
      "totally 5.05111582767128\n",
      "touch 5.824305715904762\n",
      "touches 6.922918004572872\n",
      "touching 6.229770824012927\n",
      "tough 6.922918004572872\n",
      "towards 6.517452896464707\n",
      "towers 6.922918004572872\n",
      "townsend 6.922918004572872\n",
      "track 5.824305715904762\n",
      "tract 6.006627272698717\n",
      "traditional 6.922918004572872\n",
      "traffic 6.922918004572872\n",
      "trailer 6.922918004572872\n",
      "train 6.517452896464707\n",
      "tranquillity 6.922918004572872\n",
      "transcend 6.922918004572872\n",
      "transfers 6.922918004572872\n",
      "translate 6.922918004572872\n",
      "translating 6.922918004572872\n",
      "trap 6.922918004572872\n",
      "trash 6.006627272698717\n",
      "trashy 6.922918004572872\n",
      "treachery 6.922918004572872\n",
      "treasure 6.922918004572872\n",
      "treat 6.229770824012927\n",
      "treatments 6.922918004572872\n",
      "trek 6.922918004572872\n",
      "tremendous 6.229770824012927\n",
      "tremendously 6.517452896464707\n",
      "tries 6.922918004572872\n",
      "trilogy 6.922918004572872\n",
      "trinity 6.922918004572872\n",
      "trip 6.922918004572872\n",
      "triumphed 6.922918004572872\n",
      "trond 6.922918004572872\n",
      "trooper 6.922918004572872\n",
      "trouble 6.922918004572872\n",
      "truck 6.517452896464707\n",
      "true 6.006627272698717\n",
      "truly 5.418840607796598\n",
      "trumbull 6.922918004572872\n",
      "trumpeter 6.922918004572872\n",
      "truth 6.922918004572872\n",
      "try 4.843476462893037\n",
      "trying 6.006627272698717\n",
      "trysts 6.922918004572872\n",
      "tsunami 6.922918004572872\n",
      "tuneful 6.922918004572872\n",
      "turkey 6.922918004572872\n",
      "turn 5.3134800921387715\n",
      "turned 6.517452896464707\n",
      "turns 6.517452896464707\n",
      "tv 5.536623643452981\n",
      "twice 6.517452896464707\n",
      "twirling 6.922918004572872\n",
      "twist 6.229770824012927\n",
      "twists 6.922918004572872\n",
      "two 5.218169912334447\n",
      "tying 6.922918004572872\n",
      "type 6.006627272698717\n",
      "typical 6.517452896464707\n",
      "ue 3.6842395524084917\n",
      "ugliest 6.922918004572872\n",
      "ugly 6.922918004572872\n",
      "uhura 6.922918004572872\n",
      "ultra 6.922918004572872\n",
      "um 3.9784790254064317\n",
      "unaccompanied 6.922918004572872\n",
      "unbearable 6.922918004572872\n",
      "unbearably 6.922918004572872\n",
      "unbelievable 6.006627272698717\n",
      "uncalled 6.922918004572872\n",
      "unconditional 6.922918004572872\n",
      "unconvincing 6.229770824012927\n",
      "underacting 6.922918004572872\n",
      "underappreciated 6.922918004572872\n",
      "underbite 6.922918004572872\n",
      "underlines 6.922918004572872\n",
      "underlying 6.517452896464707\n",
      "underneath 6.517452896464707\n",
      "underrated 6.922918004572872\n",
      "understand 5.218169912334447\n",
      "understanding 6.922918004572872\n",
      "understated 6.517452896464707\n",
      "understatement 6.922918004572872\n",
      "understood 6.922918004572872\n",
      "undertone 6.922918004572872\n",
      "underwater 6.922918004572872\n",
      "undoubtedly 6.517452896464707\n",
      "uneasy 6.922918004572872\n",
      "unemployed 6.922918004572872\n",
      "unethical 6.922918004572872\n",
      "unfaithful 6.922918004572872\n",
      "unfolds 6.517452896464707\n",
      "unforgettable 6.922918004572872\n",
      "unfortunate 5.824305715904762\n",
      "unfortunately 6.006627272698717\n",
      "unfunny 6.517452896464707\n",
      "unintentionally 6.922918004572872\n",
      "uninteresting 6.517452896464707\n",
      "union 6.922918004572872\n",
      "unique 6.517452896464707\n",
      "uniqueness 6.922918004572872\n",
      "universal 6.922918004572872\n",
      "universe 6.922918004572872\n",
      "unless 6.922918004572872\n",
      "unlockable 6.922918004572872\n",
      "unmatched 6.922918004572872\n",
      "unmitigated 6.922918004572872\n",
      "unmoving 6.922918004572872\n",
      "unnecessary 6.922918004572872\n",
      "unneeded 6.922918004572872\n",
      "unoriginal 6.922918004572872\n",
      "unpleasant 6.922918004572872\n",
      "unpredictability 6.922918004572872\n",
      "unpredictable 6.517452896464707\n",
      "unrealistic 6.922918004572872\n",
      "unrecognizable 6.517452896464707\n",
      "unrecommended 6.922918004572872\n",
      "unremarkable 6.922918004572872\n",
      "unrestrained 6.922918004572872\n",
      "unsatisfactory 6.922918004572872\n",
      "unwatchable 6.922918004572872\n",
      "upa 6.922918004572872\n",
      "uplifting 6.517452896464707\n",
      "upper 6.922918004572872\n",
      "ups 6.006627272698717\n",
      "uptight 6.922918004572872\n",
      "ursula 6.922918004572872\n",
      "us 2.7640349212132\n",
      "use 4.182077980647671\n",
      "used 5.3134800921387715\n",
      "user 6.922918004572872\n",
      "uses 5.824305715904762\n",
      "using 6.229770824012927\n",
      "ussr 6.922918004572872\n",
      "usual 6.006627272698717\n",
      "utter 5.824305715904762\n",
      "utterly 6.006627272698717\n",
      "valentine 6.922918004572872\n",
      "value 6.006627272698717\n",
      "values 6.229770824012927\n",
      "vampire 6.922918004572872\n",
      "vandiver 6.922918004572872\n",
      "variation 6.922918004572872\n",
      "vehicles 6.922918004572872\n",
      "ventura 6.922918004572872\n",
      "verbal 6.922918004572872\n",
      "verbatim 6.922918004572872\n",
      "versatile 6.922918004572872\n",
      "version 6.517452896464707\n",
      "versus 6.922918004572872\n",
      "vessel 6.922918004572872\n",
      "veteran 6.922918004572872\n",
      "vey 6.517452896464707\n",
      "vibe 6.922918004572872\n",
      "victor 6.922918004572872\n",
      "video 6.517452896464707\n",
      "view 5.05111582767128\n",
      "viewer 6.006627272698717\n",
      "viewing 6.517452896464707\n",
      "views 6.517452896464707\n",
      "villain 6.517452896464707\n",
      "villains 6.922918004572872\n",
      "violence 6.922918004572872\n",
      "violin 6.922918004572872\n",
      "virtue 6.922918004572872\n",
      "virus 6.922918004572872\n",
      "vision 6.517452896464707\n",
      "visual 5.824305715904762\n",
      "visually 6.517452896464707\n",
      "vitally 6.922918004572872\n",
      "vivian 6.922918004572872\n",
      "vivid 6.922918004572872\n",
      "vocal 6.922918004572872\n",
      "voice 6.517452896464707\n",
      "volatile 6.922918004572872\n",
      "volcano 6.517452896464707\n",
      "vomit 6.229770824012927\n",
      "vomited 6.922918004572872\n",
      "voyage 6.922918004572872\n",
      "vulcan 6.922918004572872\n",
      "waiting 6.922918004572872\n",
      "waitress 6.922918004572872\n",
      "walk 6.229770824012927\n",
      "walked 6.517452896464707\n",
      "wall 6.517452896464707\n",
      "want 5.218169912334447\n",
      "wanted 6.517452896464707\n",
      "wanting 6.922918004572872\n",
      "wants 6.922918004572872\n",
      "war 4.671626205966376\n",
      "warmth 6.517452896464707\n",
      "warn 6.517452896464707\n",
      "warning 6.922918004572872\n",
      "wartime 6.922918004572872\n",
      "warts 6.922918004572872\n",
      "washed 6.922918004572872\n",
      "washing 6.922918004572872\n",
      "waste 4.671626205966376\n",
      "wasted 5.824305715904762\n",
      "waster 6.922918004572872\n",
      "wasting 6.922918004572872\n",
      "watch 3.9024931184285094\n",
      "watchable 6.229770824012927\n",
      "watched 5.418840607796598\n",
      "watching 4.671626205966376\n",
      "water 6.517452896464707\n",
      "watkins 6.517452896464707\n",
      "watson 6.517452896464707\n",
      "wave 6.922918004572872\n",
      "way 4.0325462466767075\n",
      "waylaid 6.922918004572872\n",
      "wayne 6.922918004572872\n",
      "ways 5.670155036077504\n",
      "wb 6.517452896464707\n",
      "weak 6.229770824012927\n",
      "weaker 6.922918004572872\n",
      "weariness 6.922918004572872\n",
      "weaving 6.922918004572872\n",
      "website 6.922918004572872\n",
      "wedding 6.922918004572872\n",
      "weight 6.922918004572872\n",
      "weird 6.517452896464707\n",
      "well 4.060717123643403\n",
      "welsh 6.922918004572872\n",
      "went 6.922918004572872\n",
      "whatever 5.670155036077504\n",
      "whatsoever 6.229770824012927\n",
      "whenever 6.922918004572872\n",
      "whether 6.229770824012927\n",
      "whine 6.922918004572872\n",
      "whiny 6.517452896464707\n",
      "white 5.3134800921387715\n",
      "whites 6.922918004572872\n",
      "whoever 6.922918004572872\n",
      "whole 5.418840607796598\n",
      "wholesome 6.517452896464707\n",
      "wide 6.922918004572872\n",
      "widmark 6.922918004572872\n",
      "wife 6.922918004572872\n",
      "wih 6.922918004572872\n",
      "wild 6.922918004572872\n",
      "wilkinson 6.229770824012927\n",
      "william 6.922918004572872\n",
      "willie 6.922918004572872\n",
      "wily 6.922918004572872\n",
      "win 5.131158535344817\n",
      "wind 6.229770824012927\n",
      "wise 6.517452896464707\n",
      "wish 6.229770824012927\n",
      "within 6.922918004572872\n",
      "without 6.006627272698717\n",
      "witticisms 6.922918004572872\n",
      "witty 6.517452896464707\n",
      "woa 6.922918004572872\n",
      "women 6.229770824012927\n",
      "wonder 4.438011354784871\n",
      "wondered 6.922918004572872\n",
      "wonderful 4.525022731774501\n",
      "wonderfully 6.229770824012927\n",
      "wong 6.922918004572872\n",
      "wont 6.922918004572872\n",
      "woo 5.824305715904762\n",
      "wooden 6.517452896464707\n",
      "word 5.3134800921387715\n",
      "words 6.006627272698717\n",
      "work 4.320228319128488\n",
      "worked 6.922918004572872\n",
      "working 6.517452896464707\n",
      "works 5.824305715904762\n",
      "world 5.824305715904762\n",
      "worry 6.922918004572872\n",
      "worse 5.218169912334447\n",
      "worst 5.218169912334447\n",
      "worth 4.620332911578826\n",
      "worthless 6.922918004572872\n",
      "worthwhile 6.922918004572872\n",
      "worthy 6.229770824012927\n",
      "would 4.248769355146344\n",
      "wouldnt 6.922918004572872\n",
      "woven 6.922918004572872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wow 6.922918004572872\n",
      "wrap 6.922918004572872\n",
      "write 5.536623643452981\n",
      "writer 5.536623643452981\n",
      "writers 6.922918004572872\n",
      "writing 4.9770078555175585\n",
      "written 5.536623643452981\n",
      "wrong 6.229770824012927\n",
      "wrote 6.922918004572872\n",
      "yardley 6.922918004572872\n",
      "yawn 6.922918004572872\n",
      "yeah 6.517452896464707\n",
      "year 4.843476462893037\n",
      "years 5.05111582767128\n",
      "yelps 6.922918004572872\n",
      "yes 5.536623643452981\n",
      "yet 5.824305715904762\n",
      "young 5.824305715904762\n",
      "younger 6.922918004572872\n",
      "youthful 6.922918004572872\n",
      "youtube 6.922918004572872\n",
      "yun 6.922918004572872\n",
      "zillion 6.922918004572872\n",
      "zombie 6.229770824012927\n",
      "zombiez 6.922918004572872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aailiyah', 'abandoned', 'ability', 'abroad', 'absolutely', 'abstruse', 'abysmal', 'academy', 'accents', 'accessible', 'acclaimed', 'accolades', 'accurate', 'accurately', 'accused', 'achievement', 'achille', 'ackerman', 'act', 'acted', 'acting', 'action', 'actions', 'actor', 'actors', 'actress', 'actresses', 'actually', 'adams', 'adaptation', 'add', 'added', 'addition', 'admins', 'admiration', 'admitted', 'adorable', 'adrift', 'adventure', 'advise', 'aerial', 'aesthetically', 'affected', 'affleck', 'afraid', 'africa', 'afternoon', 'age', 'aged', 'ages', 'ago', 'agree', 'agreed', 'aimless', 'air', 'aired', 'akasha', 'akin', 'alert', 'alexander', 'alike', 'allison', 'allow', 'allowing', 'almost', 'along', 'alongside', 'already', 'also', 'although', 'always', 'amateurish', 'amaze', 'amazed', 'amazing', 'amazingly', 'america', 'american', 'americans', 'among', 'amount', 'amusing', 'amust', 'anatomist', 'angel', 'angela', 'angeles', 'angelina', 'angle', 'angles', 'angry', 'anguish', 'angus', 'animals', 'animated', 'animation', 'anita', 'ann', 'anne', 'anniversary', 'annoying', 'another', 'anthony', 'antithesis', 'anyone', 'anything', 'anyway', 'apart', 'appalling', 'appealing', 'appearance', 'appears', 'applauded', 'applause', 'appreciate', 'appropriate', 'apt', 'argued', 'armageddon', 'armand', 'around', 'array', 'art', 'articulated', 'artiness', 'artist', 'artistic', 'artless', 'arts', 'aside', 'ask', 'asleep', 'aspect', 'aspects', 'ass', 'assante', 'assaulted', 'assistant', 'astonishingly', 'astronaut', 'atmosphere', 'atrocious', 'atrocity', 'attempt', 'attempted', 'attempting', 'attempts', 'attention', 'attractive', 'audience', 'audio', 'aurv', 'austen', 'austere', 'author', 'average', 'aversion', 'avoid', 'avoided', 'award', 'awarded', 'awards', 'away', 'awesome', 'awful', 'awkwardly', 'aye', 'baaaaaad', 'babbling', 'babie', 'baby', 'babysitting', 'back', 'backdrop', 'backed', 'bad', 'badly', 'bag', 'bailey', 'bakery', 'balance', 'balanced', 'ball', 'ballet', 'balls', 'band', 'barcelona', 'barely', 'barking', 'barney', 'barren', 'based', 'basic', 'basically', 'bat', 'bates', 'baxendale', 'bear', 'beautiful', 'beautifully', 'bec', 'became', 'bechard', 'become', 'becomes', 'began', 'begin', 'beginning', 'behind', 'behold', 'bela', 'believable', 'believe', 'believed', 'bell', 'bellucci', 'belly', 'belmondo', 'ben', 'bendingly', 'bennett', 'bergen', 'bertolucci', 'best', 'better', 'betty', 'beware', 'beyond', 'bible', 'big', 'biggest', 'billy', 'biographical', 'bipolarity', 'bit', 'bitchy', 'black', 'blah', 'blake', 'bland', 'blandly', 'blare', 'blatant', 'blew', 'blood', 'blown', 'blue', 'blush', 'boasts', 'bob', 'body', 'bohemian', 'boiling', 'bold', 'bombardments', 'bond', 'bonding', 'bonus', 'bonuses', 'boobs', 'boogeyman', 'book', 'boost', 'bop', 'bordered', 'borderlines', 'borders', 'bore', 'bored', 'boring', 'borrowed', 'boss', 'bother', 'bothersome', 'bought', 'box', 'boyfriend', 'boyle', 'brain', 'brainsucking', 'brat', 'breaking', 'breeders', 'brevity', 'brian', 'brief', 'brigand', 'bright', 'brilliance', 'brilliant', 'brilliantly', 'bring', 'brings', 'broad', 'broke', 'brooding', 'brother', 'brutal', 'buddy', 'budget', 'buffalo', 'buffet', 'build', 'builders', 'buildings', 'built', 'bullock', 'bully', 'bunch', 'burton', 'business', 'buy', 'cable', 'cailles', 'california', 'call', 'called', 'calls', 'came', 'cameo', 'camera', 'camerawork', 'camp', 'campy', 'canada', 'cancan', 'candace', 'candle', 'cannot', 'cant', 'captain', 'captured', 'captures', 'car', 'card', 'cardboard', 'cardellini', 'care', 'carol', 'carrell', 'carries', 'carry', 'cars', 'cartoon', 'cartoons', 'case', 'cases', 'cast', 'casted', 'casting', 'cat', 'catchy', 'caught', 'cause', 'ceases', 'celebration', 'celebrity', 'celluloid', 'centers', 'central', 'century', 'certain', 'certainly', 'cg', 'cgi', 'chalkboard', 'challenges', 'chance', 'change', 'changes', 'changing', 'channel', 'character', 'characterisation', 'characters', 'charisma', 'charismatic', 'charles', 'charlie', 'charm', 'charming', 'chase', 'chasing', 'cheap', 'cheaply', 'check', 'checking', 'cheek', 'cheekbones', 'cheerfull', 'cheerless', 'cheesiness', 'cheesy', 'chemistry', 'chick', 'child', 'childhood', 'children', 'childrens', 'chills', 'chilly', 'chimp', 'chodorov', 'choice', 'choices', 'choked', 'chosen', 'chow', 'christmas', 'christopher', 'church', 'cinema', 'cinematic', 'cinematographers', 'cinematography', 'circumstances', 'class', 'classic', 'classical', 'clear', 'clearly', 'clever', 'clich', 'cliche', 'clients', 'cliff', 'climax', 'close', 'closed', 'clothes', 'club', 'co', 'coach', 'coal', 'coastal', 'coaster', 'coherent', 'cold', 'cole', 'collect', 'collective', 'colored', 'colorful', 'colours', 'columbo', 'come', 'comedic', 'comedy', 'comes', 'comfortable', 'comforting', 'comical', 'coming', 'commands', 'comment', 'commentary', 'commented', 'comments', 'commercial', 'community', 'company', 'compelling', 'competent', 'complete', 'completed', 'completely', 'complex', 'complexity', 'composed', 'composition', 'comprehensible', 'compromise', 'computer', 'concentrate', 'conception', 'conceptually', 'concerning', 'concerns', 'concert', 'conclusion', 'condescends', 'confidence', 'configuration', 'confirm', 'conflict', 'confuses', 'confusing', 'connections', 'connery', 'connor', 'conrad', 'consequences', 'consider', 'considerable', 'considered', 'considering', 'considers', 'consistent', 'consolations', 'constant', 'constantine', 'constructed', 'contained', 'containing', 'contains', 'content', 'continually', 'continuation', 'continue', 'continuity', 'continuously', 'contract', 'contrast', 'contributing', 'contributory', 'contrived', 'control', 'controversy', 'convention', 'convey', 'convince', 'convincing', 'convoluted', 'cool', 'coppola', 'cords', 'core', 'corn', 'corny', 'correct', 'cost', 'costs', 'costumes', 'cotton', 'could', 'couple', 'course', 'court', 'courtroom', 'cover', 'cowardice', 'cox', 'crackles', 'crafted', 'crap', 'crash', 'crashed', 'crayon', 'crayons', 'crazy', 'create', 'created', 'creates', 'creative', 'creativity', 'creature', 'credible', 'credit', 'credits', 'crew', 'crime', 'crisp', 'critic', 'critical', 'crocdodile', 'crocs', 'cross', 'crowd', 'crowe', 'cruel', 'cruise', 'cry', 'cult', 'culture', 'curtain', 'custer', 'cute', 'cutest', 'cutie', 'cutouts', 'cuts', 'cutting', 'dads', 'damian', 'damn', 'dance', 'dancing', 'dangerous', 'dark', 'darren', 'daughter', 'daughters', 'day', 'days', 'de', 'dead', 'deadly', 'deadpan', 'deal', 'dealt', 'death', 'debated', 'debbie', 'debits', 'debut', 'decay', 'decent', 'decidely', 'decipher', 'decisions', 'dedication', 'dee', 'deep', 'deeply', 'defensemen', 'defined', 'definitely', 'delete', 'delight', 'delightful', 'delights', 'deliver', 'delivered', 'delivering', 'delivers', 'dependant', 'depending', 'depends', 'depicted', 'depicts', 'depressing', 'depth', 'derivative', 'describe', 'describes', 'desert', 'deserved', 'deserves', 'deserving', 'design', 'designed', 'designer', 'desperately', 'desperation', 'despised', 'despite', 'destroy', 'detailing', 'details', 'develop', 'development', 'developments', 'di', 'diabetic', 'dialog', 'dialogs', 'dialogue', 'diaper', 'dickens', 'difference', 'different', 'dignity', 'dimensional', 'direct', 'directed', 'directing', 'direction', 'director', 'directorial', 'directors', 'disappointed', 'disappointing', 'disappointment', 'disaster', 'disbelief', 'discomfort', 'discovering', 'discovery', 'disgrace', 'disgusting', 'dislike', 'disliked', 'disney', 'disparate', 'distant', 'distinction', 'distorted', 'distract', 'distressed', 'disturbing', 'diving', 'doctor', 'documentaries', 'documentary', 'dodge', 'dogs', 'dollars', 'dominated', 'done', 'donlevy', 'dont', 'doomed', 'dose', 'doubt', 'downs', 'dozen', 'dr', 'dracula', 'draft', 'drag', 'drago', 'drama', 'dramatic', 'drawings', 'drawn', 'dream', 'dreams', 'dreary', 'dribble', 'drift', 'drifting', 'drive', 'drooling', 'dropped', 'dry', 'due', 'duet', 'dull', 'dumb', 'dumbest', 'duper', 'duris', 'dustin', 'dvd', 'dwight', 'dysfunction', 'earlier', 'early', 'earth', 'easily', 'easy', 'eating', 'ebay', 'ebola', 'eccleston', 'ed', 'edge', 'editing', 'edition', 'educational', 'edward', 'effect', 'effective', 'effects', 'effort', 'efforts', 'egotism', 'eighth', 'eiko', 'either', 'elaborately', 'elderly', 'elegant', 'element', 'elias', 'eloquently', 'else', 'elsewhere', 'embarrassed', 'embarrassing', 'embassy', 'emerge', 'emilio', 'emily', 'emoting', 'emotion', 'emotionally', 'emotions', 'emperor', 'empowerment', 'emptiness', 'empty', 'en', 'enchanting', 'end', 'endearing', 'ended', 'ending', 'endlessly', 'ends', 'energetic', 'energy', 'engaging', 'english', 'enhanced', 'enjoy', 'enjoyable', 'enjoyed', 'enjoyment', 'enough', 'enter', 'enterprise', 'entertained', 'entertaining', 'entire', 'entirely', 'entrance', 'episode', 'episodes', 'equivalent', 'era', 'errol', 'errors', 'escalating', 'escapism', 'especially', 'essence', 'establish', 'established', 'estate', 'estevez', 'etc', 'european', 'evaluate', 'even', 'events', 'ever', 'every', 'everybody', 'everyone', 'everything', 'everywhere', 'evidently', 'evil', 'evinced', 'evokes', 'exactly', 'exaggerating', 'example', 'excellent', 'excellently', 'except', 'exceptional', 'exceptionally', 'excerpts', 'excessively', 'exchange', 'exciting', 'excruciatingly', 'excuse', 'excuses', 'executed', 'exemplars', 'existent', 'existential', 'expansive', 'expect', 'expectations', 'expected', 'expecting', 'experience', 'experiences', 'expert', 'explain', 'explains', 'explanation', 'exploit', 'explorations', 'explosion', 'expression', 'exquisite', 'extant', 'exteriors', 'extraneous', 'extraordinary', 'extremely', 'eye', 'eyes', 'fabulous', 'face', 'faces', 'facial', 'facing', 'fact', 'factory', 'failed', 'fails', 'fair', 'fairly', 'faithful', 'fall', 'falling', 'falls', 'falsely', 'falwell', 'fame', 'famed', 'family', 'famous', 'fan', 'fanciful', 'fans', 'fantastic', 'fantasy', 'far', 'farce', 'fare', 'fascinated', 'fascinating', 'fascination', 'fashioned', 'fast', 'faster', 'fat', 'father', 'faultless', 'fausa', 'faux', 'favorite', 'favourite', 'fear', 'feature', 'features', 'feel', 'feeling', 'feelings', 'feet', 'feisty', 'fellowes', 'felt', 'female', 'females', 'ferry', 'fest', 'fi', 'fields', 'fifteen', 'fifties', 'fill', 'film', 'filmed', 'filmiing', 'filmmaker', 'filmography', 'films', 'final', 'finale', 'finally', 'financial', 'find', 'finds', 'fine', 'finest', 'fingernails', 'finished', 'fire', 'first', 'fish', 'fishnet', 'fisted', 'fit', 'five', 'flag', 'flakes', 'flaming', 'flashbacks', 'flat', 'flaw', 'flawed', 'flaws', 'fleshed', 'flick', 'flicks', 'florida', 'flowed', 'flying', 'flynn', 'focus', 'fodder', 'follow', 'following', 'follows', 'foolish', 'footage', 'football', 'force', 'forced', 'forces', 'ford', 'foreign', 'foreigner', 'forever', 'forget', 'forgettable', 'forgetting', 'forgot', 'forgotten', 'form', 'format', 'former', 'fort', 'forth', 'forwarded', 'found', 'four', 'fox', 'foxx', 'frances', 'francis', 'frankly', 'free', 'freedom', 'freeman', 'french', 'fresh', 'freshness', 'friends', 'friendship', 'frightening', 'front', 'frontier', 'frost', 'frustration', 'fulci', 'fulfilling', 'full', 'fully', 'fumbling', 'fun', 'function', 'fundamental', 'funniest', 'funny', 'future', 'fx', 'gabriel', 'gadget', 'gain', 'gake', 'galley', 'gallon', 'game', 'games', 'garage', 'garbage', 'garbo', 'garfield', 'gas', 'gaudi', 'gave', 'gay', 'geek', 'gem', 'general', 'generally', 'generates', 'generic', 'genius', 'genre', 'gently', 'genuine', 'george', 'gerardo', 'gere', 'get', 'gets', 'getting', 'ghibili', 'giallo', 'gibberish', 'gifted', 'giovanni', 'girl', 'girlfriend', 'girls', 'girolamo', 'give', 'given', 'gives', 'giving', 'glad', 'glance', 'glasses', 'gloriously', 'go', 'goalies', 'god', 'goes', 'going', 'gone', 'gonna', 'good', 'gore', 'goremeister', 'gorman', 'gosh', 'got', 'goth', 'gotta', 'gotten', 'government', 'grace', 'grade', 'gradually', 'grainy', 'granted', 'graphics', 'grasp', 'grates', 'great', 'greatest', 'greatness', 'green', 'greenstreet', 'grew', 'grim', 'grimes', 'gripping', 'groove', 'gross', 'ground', 'guards', 'guess', 'guests', 'guilt', 'gung', 'guy', 'guys', 'hackneyed', 'haggis', 'hair', 'hairsplitting', 'half', 'halfway', 'ham', 'hand', 'handle', 'handled', 'handles', 'hands', 'hang', 'hankies', 'hanks', 'happen', 'happened', 'happiness', 'happy', 'hard', 'harris', 'hate', 'hated', 'hatred', 'havilland', 'hay', 'hayao', 'hayworth', 'hbo', 'head', 'heads', 'hear', 'heard', 'heart', 'hearts', 'heartwarming', 'heaven', 'heche', 'heels', 'heist', 'helen', 'hell', 'hellish', 'helms', 'help', 'helping', 'helps', 'hence', 'hendrikson', 'hernandez', 'hero', 'heroes', 'heroine', 'heroism', 'hes', 'hide', 'high', 'higher', 'highest', 'highlights', 'highly', 'hilarious', 'hill', 'hilt', 'hip', 'history', 'hitchcock', 'ho', 'hockey', 'hoffman', 'hold', 'holding', 'holds', 'holes', 'hollander', 'hollow', 'hollywood', 'home', 'homework', 'honest', 'honestly', 'hoot', 'hope', 'hopefully', 'hopeless', 'horrendous', 'horrendously', 'horrible', 'horrid', 'horrified', 'horror', 'horse', 'hosting', 'hot', 'hour', 'hours', 'house', 'houses', 'howdy', 'howe', 'howell', 'however', 'huge', 'hugo', 'human', 'humanity', 'humans', 'hummh', 'humor', 'humorous', 'humour', 'hurt', 'huston', 'hype', 'hypocrisy', 'idea', 'idealogical', 'identified', 'identifies', 'identify', 'idiot', 'idiotic', 'idyllic', 'iffy', 'im', 'imaginable', 'imagination', 'imaginative', 'imagine', 'imdb', 'imitation', 'impact', 'imperial', 'implausible', 'important', 'impossible', 'impressed', 'impression', 'impressive', 'improved', 'improvement', 'improvisation', 'impulse', 'inappropriate', 'incendiary', 'includes', 'including', 'incomprehensible', 'inconsistencies', 'incorrectness', 'incredible', 'incredibly', 'indeed', 'indescribably', 'indication', 'indictment', 'indie', 'individual', 'indoor', 'indulgent', 'industry', 'ineptly', 'inexperience', 'inexplicable', 'initially', 'innocence', 'insane', 'inside', 'insincere', 'insipid', 'insomniacs', 'inspiration', 'inspiring', 'instant', 'instead', 'instruments', 'insulin', 'insult', 'intangibles', 'integral', 'integration', 'intelligence', 'intelligent', 'intense', 'intensity', 'intentions', 'interacting', 'interest', 'interested', 'interesting', 'interim', 'interplay', 'interpretations', 'interview', 'intoning', 'intrigued', 'inventive', 'involved', 'involves', 'involving', 'iq', 'ireland', 'ironically', 'irons', 'ironside', 'irritating', 'ishioka', 'iso', 'issue', 'issues', 'istagey', 'italian', 'ive', 'jack', 'jaclyn', 'james', 'jamie', 'japanese', 'jason', 'jay', 'jealousy', 'jean', 'jennifer', 'jerky', 'jerry', 'jessica', 'jessice', 'jet', 'jim', 'jimmy', 'job', 'jobs', 'joe', 'john', 'joins', 'joke', 'jokes', 'jonah', 'jones', 'journey', 'joy', 'joyce', 'juano', 'judge', 'judging', 'judith', 'judo', 'julian', 'june', 'junk', 'junkyard', 'justice', 'jutland', 'kanaly', 'kathy', 'keep', 'keeps', 'keira', 'keith', 'kept', 'kevin', 'kid', 'kidnapped', 'kids', 'kieslowski', 'kill', 'killer', 'killing', 'killings', 'kind', 'kinda', 'kirk', 'kitchy', 'knew', 'knightley', 'knocked', 'know', 'known', 'knows', 'koteas', 'kris', 'kristoffersen', 'kudos', 'la', 'labute', 'lack', 'lacked', 'lacks', 'ladies', 'lady', 'lame', 'lance', 'landscapes', 'lane', 'lange', 'largely', 'laselva', 'lassie', 'last', 'lasting', 'latched', 'late', 'later', 'latest', 'latifa', 'latin', 'latter', 'laugh', 'laughable', 'laughed', 'laughs', 'layers', 'lazy', 'lead', 'leading', 'leap', 'learn', 'least', 'leave', 'leaves', 'leaving', 'lee', 'left', 'legal', 'legendary', 'length', 'leni', 'less', 'lesser', 'lestat', 'let', 'lets', 'letting', 'level', 'levels', 'lewis', 'lid', 'lie', 'lies', 'lieutenant', 'life', 'lifetime', 'light', 'lighting', 'like', 'liked', 'likes', 'lilli', 'lilt', 'limitations', 'limited', 'linda', 'line', 'linear', 'lines', 'lino', 'lion', 'list', 'literally', 'littered', 'little', 'lived', 'lives', 'living', 'loads', 'local', 'location', 'locations', 'loewenhielm', 'logic', 'london', 'loneliness', 'long', 'longer', 'look', 'looked', 'looking', 'looks', 'loose', 'loosely', 'lord', 'los', 'losing', 'lost', 'lot', 'lots', 'lousy', 'lovable', 'love', 'loved', 'lovely', 'loves', 'low', 'lower', 'loyalty', 'lucio', 'lucy', 'lugosi', 'lust', 'luv', 'lyrics', 'macbeth', 'machine', 'mad', 'made', 'magnificent', 'main', 'mainly', 'major', 'make', 'maker', 'makers', 'makes', 'making', 'male', 'males', 'malta', 'man', 'managed', 'manages', 'manna', 'mansonites', 'many', 'marbles', 'march', 'marine', 'marion', 'mark', 'marred', 'marriage', 'martin', 'masculine', 'masculinity', 'massive', 'master', 'masterful', 'masterpiece', 'masterpieces', 'material', 'matrix', 'matter', 'matthews', 'mature', 'may', 'maybe', 'mchattie', 'mclaglen', 'meagre', 'mean', 'meanders', 'meaning', 'meanings', 'meant', 'medical', 'mediocre', 'meld', 'melodrama', 'melville', 'member', 'members', 'memorable', 'memories', 'memorized', 'menace', 'menacing', 'mention', 'mercy', 'meredith', 'merit', 'mesmerising', 'mess', 'messages', 'meteorite', 'mexican', 'michael', 'mickey', 'microsoft', 'middle', 'might', 'mighty', 'mind', 'mindblowing', 'miner', 'mini', 'minor', 'minute', 'minutes', 'mirrormask', 'miserable', 'miserably', 'mishima', 'misplace', 'miss', 'missed', 'mistakes', 'miyazaki', 'modern', 'modest', 'mollusk', 'moment', 'moments', 'momentum', 'money', 'monica', 'monolog', 'monotonous', 'monster', 'monstrous', 'monumental', 'moral', 'morgan', 'morons', 'mostly', 'mother', 'motion', 'motivations', 'mountain', 'mouse', 'mouth', 'move', 'moved', 'movements', 'moves', 'movie', 'movies', 'moving', 'ms', 'much', 'muddled', 'muppets', 'murder', 'murdered', 'murdering', 'murky', 'music', 'musician', 'must', 'mystifying', 'naked', 'narration', 'narrative', 'nasty', 'national', 'nationalities', 'native', 'natural', 'nature', 'naughty', 'nearly', 'necklace', 'need', 'needed', 'needlessly', 'negative', 'negulesco', 'neighbour', 'neil', 'nerves', 'nervous', 'net', 'netflix', 'network', 'never', 'nevertheless', 'nevsky', 'new', 'next', 'nice', 'nicola', 'night', 'nimoy', 'nine', 'no', 'noble', 'nobody', 'noir', 'non', 'none', 'nonetheless', 'nonsense', 'nor', 'normally', 'northern', 'nostalgia', 'not', 'notable', 'notch', 'note', 'noteworthy', 'nothing', 'novella', 'number', 'numbers', 'nun', 'nuns', 'nurse', 'nut', 'nuts', 'obliged', 'obsessed', 'obvious', 'obviously', 'occasionally', 'occupied', 'occur', 'occurs', 'odd', 'offend', 'offensive', 'offer', 'offers', 'often', 'oh', 'okay', 'old', 'olde', 'older', 'ole', 'olivia', 'omit', 'one', 'ones', 'open', 'opened', 'opening', 'operas', 'opinion', 'ordeal', 'oriented', 'original', 'originality', 'origins', 'ortolani', 'oscar', 'others', 'otherwise', 'ought', 'outlandish', 'outlets', 'outside', 'outward', 'overacting', 'overall', 'overcome', 'overdue', 'overly', 'overs', 'overt', 'overwrought', 'owed', 'owls', 'owned', 'owns', 'oy', 'pace', 'paced', 'pacing', 'pack', 'paid', 'painful', 'painfully', 'paint', 'painted', 'pair', 'palance', 'pandering', 'pans', 'paolo', 'pap', 'paper', 'par', 'parents', 'park', 'parker', 'part', 'partaking', 'particular', 'particularly', 'parts', 'passed', 'passion', 'past', 'patent', 'pathetic', 'patriotism', 'paul', 'pay', 'peaking', 'pearls', 'peculiarity', 'pedestal', 'pencil', 'people', 'perabo', 'perfect', 'perfected', 'perfectly', 'performance', 'performances', 'perhaps', 'period', 'perplexing', 'person', 'personalities', 'personally', 'peter', 'pg', 'phantasm', 'phenomenal', 'philippa', 'phony', 'photograph', 'photography', 'phrase', 'physical', 'pi', 'picked', 'picture', 'pictures', 'piece', 'pieces', 'pile', 'pillow', 'pitch', 'pitiful', 'pixar', 'place', 'places', 'plain', 'plane', 'planned', 'plants', 'play', 'played', 'player', 'players', 'playing', 'plays', 'pleasant', 'pleased', 'pleaser', 'pleasing', 'pledge', 'plenty', 'plmer', 'plot', 'plug', 'plus', 'pm', 'poet', 'poetry', 'poignant', 'point', 'pointillistic', 'pointless', 'poised', 'poler', 'political', 'politically', 'politics', 'ponyo', 'poor', 'poorly', 'popcorn', 'popular', 'portrayal', 'portrayals', 'portrayed', 'portraying', 'positive', 'possible', 'possibly', 'post', 'potted', 'power', 'powerful', 'powerhouse', 'practical', 'practically', 'pray', 'precisely', 'predict', 'predictable', 'predictably', 'prejudice', 'prelude', 'premise', 'prepared', 'presence', 'presents', 'preservation', 'president', 'pretentious', 'pretext', 'pretty', 'previous', 'primal', 'primary', 'probably', 'problem', 'problems', 'proceedings', 'process', 'produce', 'produced', 'producer', 'producers', 'product', 'production', 'professionals', 'professor', 'progresses', 'promote', 'prompted', 'prone', 'propaganda', 'properly', 'proud', 'proudly', 'provided', 'provokes', 'provoking', 'ps', 'pseudo', 'psychological', 'psychotic', 'public', 'pull', 'pulling', 'pulls', 'punched', 'punches', 'punish', 'punishment', 'puppet', 'puppets', 'pure', 'purity', 'put', 'putting', 'puzzle', 'pyromaniac', 'qu', 'quaid', 'qualities', 'quality', 'question', 'questioning', 'quick', 'quicker', 'quiet', 'quinn', 'quite', 'race', 'racial', 'racism', 'radiant', 'raging', 'random', 'range', 'ranks', 'rare', 'rate', 'rated', 'rather', 'rating', 'ratings', 'raver', 'raw', 'ray', 'reactions', 'readers', 'reading', 'ready', 'real', 'realised', 'realistic', 'reality', 'realize', 'realized', 'really', 'reason', 'reasonable', 'reasons', 'receive', 'received', 'recent', 'recently', 'recommend', 'recommended', 'reconciliation', 'recover', 'recurring', 'redeemed', 'redeeming', 'reenactments', 'references', 'reflected', 'refreshing', 'regardless', 'regret', 'regrettable', 'regrettably', 'rejection', 'relate', 'related', 'relation', 'relations', 'relationship', 'relationships', 'relatively', 'relaxing', 'release', 'released', 'relief', 'relying', 'remaining', 'remake', 'remarkable', 'remember', 'reminded', 'remotely', 'removing', 'rendering', 'rendition', 'renowned', 'rent', 'repair', 'repeated', 'repeating', 'repeats', 'repertory', 'reporter', 'represents', 'require', 'rescue', 'researched', 'resounding', 'respecting', 'rest', 'restrained', 'result', 'results', 'resume', 'retarded', 'retreat', 'return', 'revealing', 'revenge', 'revere', 'reverse', 'review', 'reviewer', 'reviewers', 'reviews', 'rice', 'rickman', 'ridiculous', 'ridiculousness', 'right', 'riot', 'rips', 'rise', 'rita', 'rivalry', 'riveted', 'riz', 'road', 'robert', 'robotic', 'rochon', 'rocked', 'rocks', 'roeg', 'role', 'roles', 'roller', 'rolls', 'romantic', 'room', 'roosevelt', 'roth', 'rough', 'round', 'routine', 'row', 'rpg', 'rpger', 'rubbish', 'rubin', 'rumbles', 'run', 'running', 'ruthless', 'ryan', 'ryans', 'sabotages', 'sack', 'sacrifice', 'sad', 'said', 'sake', 'salesman', 'sam', 'sample', 'sand', 'sandra', 'sappiest', 'sarcophage', 'sat', 'satanic', 'savalas', 'savant', 'save', 'savor', 'saw', 'say', 'says', 'scale', 'scamp', 'scare', 'scared', 'scares', 'scary', 'scene', 'scenery', 'scenes', 'schilling', 'schizophrenic', 'school', 'schoolers', 'schrader', 'schultz', 'sci', 'science', 'scientist', 'score', 'scot', 'scream', 'screamy', 'screen', 'screened', 'screenplay', 'screenwriter', 'scrimm', 'script', 'scripted', 'scripting', 'scripts', 'sculpture', 'sea', 'seamless', 'seamlessly', 'sean', 'season', 'seat', 'second', 'secondary', 'secondly', 'see', 'seeing', 'seem', 'seemed', 'seems', 'seen', 'selections', 'self', 'sells', 'semi', 'senior', 'sense', 'senses', 'sensibility', 'sensitivities', 'sentiment', 'seperate', 'sequel', 'sequels', 'sequence', 'sequences', 'series', 'serious', 'seriously', 'served', 'set', 'sets', 'setting', 'settings', 'seuss', 'several', 'sex', 'shakespear', 'shakespears', 'shallow', 'shame', 'shameful', 'share', 'sharing', 'sharply', 'shatner', 'shattered', 'shed', 'sheer', 'shelf', 'shell', 'shelves', 'shenanigans', 'shepard', 'shined', 'shirley', 'shocking', 'shooting', 'short', 'shortlist', 'shot', 'shots', 'show', 'showcasing', 'showed', 'shows', 'shut', 'sibling', 'sick', 'side', 'sidelined', 'sign', 'significant', 'silent', 'silly', 'simmering', 'simplifying', 'simply', 'since', 'sincere', 'sing', 'singing', 'single', 'sinister', 'sink', 'sinking', 'sister', 'sisters', 'sit', 'sitcoms', 'site', 'sites', 'sits', 'situation', 'situations', 'skilled', 'skip', 'slackers', 'slavic', 'sleep', 'slideshow', 'slightest', 'slightly', 'slimy', 'sloppy', 'slow', 'slurs', 'smack', 'small', 'smart', 'smells', 'smile', 'smiling', 'smith', 'smoothly', 'snider', 'snow', 'soap', 'sobering', 'social', 'soldiers', 'sole', 'solid', 'solidifying', 'solving', 'someone', 'something', 'sometimes', 'somewhat', 'son', 'song', 'songs', 'soon', 'sophisticated', 'sorrentino', 'sorry', 'sort', 'soul', 'sound', 'sounded', 'sounds', 'soundtrack', 'sour', 'south', 'southern', 'space', 'spacek', 'spacey', 'span', 'speak', 'speaking', 'special', 'speed', 'spend', 'spent', 'spew', 'sphere', 'spiffy', 'splendid', 'spock', 'spoil', 'spoiled', 'spoiler', 'spoilers', 'spot', 'spy', 'squibs', 'stable', 'stage', 'stagy', 'stand', 'standout', 'stanwyck', 'star', 'starlet', 'starring', 'stars', 'start', 'started', 'starts', 'state', 'stay', 'stayed', 'stealing', 'steamboat', 'steele', 'step', 'stephen', 'stereotypes', 'stereotypically', 'steve', 'stewart', 'stick', 'still', 'stinker', 'stinks', 'stocking', 'stockings', 'stoic', 'store', 'stories', 'storm', 'story', 'storyline', 'storytelling', 'stowe', 'strange', 'stranger', 'stratus', 'straw', 'street', 'strident', 'string', 'strives', 'strokes', 'strong', 'struck', 'structure', 'struggle', 'stuart', 'student', 'students', 'studio', 'study', 'stuff', 'stunning', 'stupid', 'stupidity', 'style', 'stylized', 'sub', 'subject', 'subjects', 'sublime', 'sublimely', 'subplots', 'subtitles', 'subtle', 'subversive', 'subverting', 'succeeded', 'succeeds', 'success', 'suck', 'sucked', 'sucks', 'suffered', 'suffering', 'suggest', 'suggests', 'suited', 'sum', 'summary', 'sundays', 'super', 'superb', 'superbad', 'superbly', 'superficial', 'superlative', 'supernatural', 'supporting', 'supposed', 'supposedly', 'sure', 'surely', 'surf', 'surface', 'surprised', 'surprises', 'surprising', 'surprisingly', 'surrounding', 'surroundings', 'survivors', 'suspense', 'suspension', 'sven', 'swamp', 'sweep', 'sweet', 'switched', 'swords', 'sydney', 'sympathetic', 'syrupy', 'system', 'tacky', 'taelons', 'take', 'taken', 'takes', 'taking', 'tale', 'talent', 'talented', 'talents', 'talk', 'tanks', 'taped', 'tardis', 'task', 'taste', 'taxidermists', 'taylor', 'teacher', 'teaches', 'team', 'tear', 'tears', 'technically', 'teddy', 'tedium', 'teen', 'teenagers', 'teeth', 'telephone', 'television', 'tell', 'telly', 'temperaments', 'ten', 'tender', 'tension', 'tensions', 'terminology', 'terms', 'terrible', 'terribly', 'terrific', 'terror', 'th', 'thanks', 'theater', 'theatre', 'theatres', 'theatrical', 'theme', 'themes', 'therapy', 'thick', 'thing', 'things', 'think', 'thinking', 'thomerson', 'thoroughly', 'thorsen', 'though', 'thought', 'thoughts', 'thousand', 'thread', 'three', 'threshold', 'thrilled', 'thriller', 'thrillers', 'throughout', 'throwback', 'thrown', 'thug', 'thumper', 'thunderbirds', 'thus', 'ticker', 'tickets', 'tightly', 'time', 'timeless', 'timely', 'timers', 'times', 'timing', 'tiny', 'tired', 'title', 'titta', 'today', 'together', 'told', 'tolerable', 'tolerate', 'tom', 'tomorrow', 'tone', 'tongue', 'tonight', 'tons', 'tony', 'took', 'toons', 'top', 'tops', 'torture', 'tortured', 'total', 'totally', 'touch', 'touches', 'touching', 'tough', 'towards', 'towers', 'townsend', 'track', 'tract', 'traditional', 'traffic', 'trailer', 'train', 'tranquillity', 'transcend', 'transfers', 'translate', 'translating', 'trap', 'trash', 'trashy', 'treachery', 'treasure', 'treat', 'treatments', 'trek', 'tremendous', 'tremendously', 'tries', 'trilogy', 'trinity', 'trip', 'triumphed', 'trond', 'trooper', 'trouble', 'truck', 'true', 'truly', 'trumbull', 'trumpeter', 'truth', 'try', 'trying', 'trysts', 'tsunami', 'tuneful', 'turkey', 'turn', 'turned', 'turns', 'tv', 'twice', 'twirling', 'twist', 'twists', 'two', 'tying', 'type', 'typical', 'ue', 'ugliest', 'ugly', 'uhura', 'ultra', 'um', 'unaccompanied', 'unbearable', 'unbearably', 'unbelievable', 'uncalled', 'unconditional', 'unconvincing', 'underacting', 'underappreciated', 'underbite', 'underlines', 'underlying', 'underneath', 'underrated', 'understand', 'understanding', 'understated', 'understatement', 'understood', 'undertone', 'underwater', 'undoubtedly', 'uneasy', 'unemployed', 'unethical', 'unfaithful', 'unfolds', 'unforgettable', 'unfortunate', 'unfortunately', 'unfunny', 'unintentionally', 'uninteresting', 'union', 'unique', 'uniqueness', 'universal', 'universe', 'unless', 'unlockable', 'unmatched', 'unmitigated', 'unmoving', 'unnecessary', 'unneeded', 'unoriginal', 'unpleasant', 'unpredictability', 'unpredictable', 'unrealistic', 'unrecognizable', 'unrecommended', 'unremarkable', 'unrestrained', 'unsatisfactory', 'unwatchable', 'upa', 'uplifting', 'upper', 'ups', 'uptight', 'ursula', 'us', 'use', 'used', 'user', 'uses', 'using', 'ussr', 'usual', 'utter', 'utterly', 'valentine', 'value', 'values', 'vampire', 'vandiver', 'variation', 'vehicles', 'ventura', 'verbal', 'verbatim', 'versatile', 'version', 'versus', 'vessel', 'veteran', 'vey', 'vibe', 'victor', 'video', 'view', 'viewer', 'viewing', 'views', 'villain', 'villains', 'violence', 'violin', 'virtue', 'virus', 'vision', 'visual', 'visually', 'vitally', 'vivian', 'vivid', 'vocal', 'voice', 'volatile', 'volcano', 'vomit', 'vomited', 'voyage', 'vulcan', 'waiting', 'waitress', 'walk', 'walked', 'wall', 'want', 'wanted', 'wanting', 'wants', 'war', 'warmth', 'warn', 'warning', 'wartime', 'warts', 'washed', 'washing', 'waste', 'wasted', 'waster', 'wasting', 'watch', 'watchable', 'watched', 'watching', 'water', 'watkins', 'watson', 'wave', 'way', 'waylaid', 'wayne', 'ways', 'wb', 'weak', 'weaker', 'weariness', 'weaving', 'website', 'wedding', 'weight', 'weird', 'well', 'welsh', 'went', 'whatever', 'whatsoever', 'whenever', 'whether', 'whine', 'whiny', 'white', 'whites', 'whoever', 'whole', 'wholesome', 'wide', 'widmark', 'wife', 'wih', 'wild', 'wilkinson', 'william', 'willie', 'wily', 'win', 'wind', 'wise', 'wish', 'within', 'without', 'witticisms', 'witty', 'woa', 'women', 'wonder', 'wondered', 'wonderful', 'wonderfully', 'wong', 'wont', 'woo', 'wooden', 'word', 'words', 'work', 'worked', 'working', 'works', 'world', 'worry', 'worse', 'worst', 'worth', 'worthless', 'worthwhile', 'worthy', 'would', 'wouldnt', 'woven', 'wow', 'wrap', 'write', 'writer', 'writers', 'writing', 'written', 'wrong', 'wrote', 'yardley', 'yawn', 'yeah', 'year', 'years', 'yelps', 'yes', 'yet', 'young', 'younger', 'youthful', 'youtube', 'yun', 'zillion', 'zombie', 'zombiez']\n"
     ]
    }
   ],
   "source": [
    "# Applying the fit function to corpus dataset\n",
    "fit_tfidf(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping all the unique words and their tfidf values into two lists and zipping them into a dictionary\n",
    "vocab_dict=dict(zip(final_word,final_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorting this based on the values and if values are same then alphabetically. \n",
    "#Ref Link: https://stackoverflow.com/questions/613183/how-do-i-sort-a-dictionary-by-value\n",
    "sorted_vocab= (sorted(vocab_dict.items(), key = lambda x : x[1],reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Putting the top 50 features , i.e words and their values into a dictionary.\n",
    "words1=[]\n",
    "values=[]\n",
    "i=0\n",
    "j=0\n",
    "while (i<=51):\n",
    "    words1.append(sorted_vocab[i][0])\n",
    "    i+=1\n",
    "while(j<51):\n",
    "    values.append(sorted_vocab[j][1])\n",
    "    j+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aailiyah abandoned abroad abstruse academy accents accessible acclaimed accolades accurately achille ackerman adams added admins admiration admitted adrift adventure aesthetically affected affleck afternoon agreed aimless aired akasha alert alike allison allowing alongside amateurish amazed amazingly amusing amust anatomist angela angelina angry anguish angus animals animated anita anniversary anthony antithesis anyway apart appears\n",
      "6.922918004572872 6.922918004572872 6.922918004572872 6.922918004572872 6.922918004572872 6.922918004572872 6.922918004572872 6.922918004572872 6.922918004572872 6.922918004572872 6.922918004572872 6.922918004572872 6.922918004572872 6.922918004572872 6.922918004572872 6.922918004572872 6.922918004572872 6.922918004572872 6.922918004572872 6.922918004572872 6.922918004572872 6.922918004572872 6.922918004572872 6.922918004572872 6.922918004572872 6.922918004572872 6.922918004572872 6.922918004572872 6.922918004572872 6.922918004572872 6.922918004572872 6.922918004572872 6.922918004572872 6.922918004572872 6.922918004572872 6.922918004572872 6.922918004572872 6.922918004572872 6.922918004572872 6.922918004572872 6.922918004572872 6.922918004572872 6.922918004572872 6.922918004572872 6.922918004572872 6.922918004572872 6.922918004572872 6.922918004572872 6.922918004572872 6.922918004572872 6.922918004572872\n"
     ]
    }
   ],
   "source": [
    "#printing the words and their values respectively\n",
    "print(*words1)\n",
    "print(*values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary with top 51 features\n",
    "dict_50=dict(zip(words1,values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I took the top 51 values \n",
    "len(dict_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'aailiyah': 6.922918004572872, 'abandoned': 6.922918004572872, 'abroad': 6.922918004572872, 'abstruse': 6.922918004572872, 'academy': 6.922918004572872, 'accents': 6.922918004572872, 'accessible': 6.922918004572872, 'acclaimed': 6.922918004572872, 'accolades': 6.922918004572872, 'accurately': 6.922918004572872, 'achille': 6.922918004572872, 'ackerman': 6.922918004572872, 'adams': 6.922918004572872, 'added': 6.922918004572872, 'admins': 6.922918004572872, 'admiration': 6.922918004572872, 'admitted': 6.922918004572872, 'adrift': 6.922918004572872, 'adventure': 6.922918004572872, 'aesthetically': 6.922918004572872, 'affected': 6.922918004572872, 'affleck': 6.922918004572872, 'afternoon': 6.922918004572872, 'agreed': 6.922918004572872, 'aimless': 6.922918004572872, 'aired': 6.922918004572872, 'akasha': 6.922918004572872, 'alert': 6.922918004572872, 'alike': 6.922918004572872, 'allison': 6.922918004572872, 'allowing': 6.922918004572872, 'alongside': 6.922918004572872, 'amateurish': 6.922918004572872, 'amazed': 6.922918004572872, 'amazingly': 6.922918004572872, 'amusing': 6.922918004572872, 'amust': 6.922918004572872, 'anatomist': 6.922918004572872, 'angela': 6.922918004572872, 'angelina': 6.922918004572872, 'angry': 6.922918004572872, 'anguish': 6.922918004572872, 'angus': 6.922918004572872, 'animals': 6.922918004572872, 'animated': 6.922918004572872, 'anita': 6.922918004572872, 'anniversary': 6.922918004572872, 'anthony': 6.922918004572872, 'antithesis': 6.922918004572872, 'anyway': 6.922918004572872, 'apart': 6.922918004572872}\n"
     ]
    }
   ],
   "source": [
    "#the values and their values\n",
    "print(dict_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for printing the dense matrix after transformation. This will be called inside the transformation function.\n",
    "def dense_matrix(data,vocab):\n",
    "    global features \n",
    "    features=[]\n",
    "    global features_values\n",
    "    features_values=[]\n",
    "    features=words1\n",
    "    for idx,row in enumerate(tqdm(features)):\n",
    "        for words in row.split(' '):\n",
    "            if (words in corpus_input):\n",
    "                features_values.append(dict_50[words])\n",
    "            else:\n",
    "                features_values.append(0)\n",
    "    a=np.column_stack((features, features_values))\n",
    "    print('dense_matrix',a)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fucntion for transforming the fit data.\n",
    "global normalized_value\n",
    "normalized_value=[]\n",
    "global final_normalized_input\n",
    "final_normalized_input=[]\n",
    "rows = []\n",
    "columns = []\n",
    "values = []\n",
    "final_output=[]\n",
    "output=[]\n",
    "\n",
    "def tfidf_transform(data,vocab):\n",
    "    prev_idx=0\n",
    "    if isinstance(data,(list,)):\n",
    "        for idx,row in enumerate(tqdm(data)):\n",
    "            word_column=0\n",
    "            for word in row.split(' '):\n",
    "                word_column+=1\n",
    "                key_value=word_val.get(word,0)\n",
    "                \n",
    "#                 print(normalized_value)\n",
    "#                 print(idx)\n",
    "#                 print(prev_idx)\n",
    "                if(idx!=prev_idx):\n",
    "#                     print(\"normal\",normalized_value)\n",
    "                    final_normalized_input.append(normalized_value[:])\n",
    "#                     print(final_normalized_input)\n",
    "#                     print(\"prev\",prev_idx)\n",
    "#                     print(\"current\",idx)\n",
    "            \n",
    "                    prev_idx=idx\n",
    "                    normalized_value.clear()\n",
    "                normalized_value.append(key_value)\n",
    "#                 print(normalized_value)\n",
    "#                 print(\"outside_normal\",normalized_value)\n",
    "        final_normalized_input.append(normalized_value[:])\n",
    "                    \n",
    "                    \n",
    "# #                 rows.append(idx)\n",
    "# #                 columns.append(word_column)\n",
    "#                 normalized_value.append(key_value)\n",
    "# #                 print(word)\n",
    "#                 if(row!=(prev_row)):\n",
    "        \n",
    "        for i in range(len(final_normalized_input)):\n",
    "            \n",
    "            output=normalize([final_normalized_input[i]])\n",
    "            final_output.append(output[:])\n",
    "            \n",
    "        j=0\n",
    "#         for i in range(len(final_output)):\n",
    "#             for k in range(len(final_output)):\n",
    "#                 print(i,j,k,final_output[i][j][k])\n",
    "                \n",
    "            \n",
    "        print(\"sparse matrix\",final_output)\n",
    "        dense_matrix(corpus_input,dict_50)\n",
    "\n",
    "                       \n",
    "        \n",
    "        \n",
    "#                 prev_row=row\n",
    "                \n",
    "# #                 print(idx,word_column,normalized_value)\n",
    "#                 print(idx)   \n",
    "# #             final_normalized_input.append(normalized_value)\n",
    "# #             print(normalized_value)\n",
    "# # #             output=normalize(final_normalized_input)\n",
    "# #             final_output.append(output)      \n",
    "        \n",
    "        \n",
    "    else:\n",
    "        print(\"input a list\")\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['slow moving aimless movie distressed drifting young man']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one document, on which we will test our tfidf features.\n",
    "type(corpus[0])\n",
    "# As corpus[0] is a str, we take a new variable called corpus_input which is a list and append first element into it.\n",
    "corpus_input=[]\n",
    "# appending first element\n",
    "corpus_input=['slow moving aimless movie distressed drifting young man']\n",
    "print(corpus_input)\n",
    "\n",
    "\n",
    "# type of this one document is a list. Now, we can input this into the transform fuction.\n",
    "type(corpus_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparse matrix [array([[0., 0., 0., 0., 0., 0., 0., 0.]])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 52/52 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense_matrix [['aailiyah' '0']\n",
      " ['abandoned' '0']\n",
      " ['abroad' '0']\n",
      " ['abstruse' '0']\n",
      " ['academy' '0']\n",
      " ['accents' '0']\n",
      " ['accessible' '0']\n",
      " ['acclaimed' '0']\n",
      " ['accolades' '0']\n",
      " ['accurately' '0']\n",
      " ['achille' '0']\n",
      " ['ackerman' '0']\n",
      " ['adams' '0']\n",
      " ['added' '0']\n",
      " ['admins' '0']\n",
      " ['admiration' '0']\n",
      " ['admitted' '0']\n",
      " ['adrift' '0']\n",
      " ['adventure' '0']\n",
      " ['aesthetically' '0']\n",
      " ['affected' '0']\n",
      " ['affleck' '0']\n",
      " ['afternoon' '0']\n",
      " ['agreed' '0']\n",
      " ['aimless' '0']\n",
      " ['aired' '0']\n",
      " ['akasha' '0']\n",
      " ['alert' '0']\n",
      " ['alike' '0']\n",
      " ['allison' '0']\n",
      " ['allowing' '0']\n",
      " ['alongside' '0']\n",
      " ['amateurish' '0']\n",
      " ['amazed' '0']\n",
      " ['amazingly' '0']\n",
      " ['amusing' '0']\n",
      " ['amust' '0']\n",
      " ['anatomist' '0']\n",
      " ['angela' '0']\n",
      " ['angelina' '0']\n",
      " ['angry' '0']\n",
      " ['anguish' '0']\n",
      " ['angus' '0']\n",
      " ['animals' '0']\n",
      " ['animated' '0']\n",
      " ['anita' '0']\n",
      " ['anniversary' '0']\n",
      " ['anthony' '0']\n",
      " ['antithesis' '0']\n",
      " ['anyway' '0']\n",
      " ['apart' '0']\n",
      " ['appears' '0']]\n"
     ]
    }
   ],
   "source": [
    "#Calling the transform function which will print the sparse and dense matrix of the test document.\n",
    "tfidf_transform(corpus_input,dict_50)\n",
    "\n",
    "\n",
    "# The dense matrix is having 50 columns and 1 row and all the values are zero as this doesnt \n",
    "  #have any of the top 50 feature words in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment_3_Instructions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
